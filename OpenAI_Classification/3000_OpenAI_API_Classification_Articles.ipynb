{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify OpenAI library version\n",
    "print(f\"OpenAI library version: {openai.__version__}\")  # Should print 0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "openai.api_key = 'sk-proj-dvUlhrqVeqGmdn2a7b3pT3BlbkFJbWEX2j67TZIrKSZvCfzm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the file path\n",
    "base_dir = r'C:\\Users\\monbi\\OneDrive\\Documents\\BSE\\Term 3\\Masters Thesis\\ECB_Perceived_Cacophony'\n",
    "file_name = 'Articles_to_Score_3000.xlsx'\n",
    "file_path = os.path.join(base_dir, file_name)\n",
    "print(\"Loading the Excel file...\")\n",
    "articles_df = pd.read_excel(file_path)\n",
    "print(\"Excel file loaded successfully.\")\n",
    "\n",
    "# Display the head of the DataFrame to check the data\n",
    "print(\"Head of the loaded DataFrame:\")\n",
    "articles_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the DataFrame:\", articles_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classify and score function\n",
    "def classify_and_score_article(article, prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a financial expert.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt + article}\n",
    "                ],\n",
    "                max_tokens=150  # Adjust max_tokens if needed\n",
    "            )\n",
    "            result = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "            # Assign scores based on textual labels\n",
    "            if \"HAWKISH\" in result.upper():\n",
    "                score = 1\n",
    "            elif \"DOVISH\" in result.upper():\n",
    "                score = -1\n",
    "            elif \"NEUTRAL\" in result.upper():\n",
    "                score = 0\n",
    "            else:\n",
    "                raise ValueError(\"Incomplete response from API\")\n",
    "                \n",
    "            return score\n",
    "\n",
    "        except openai.error.RateLimitError:\n",
    "            print(\"Rate limit exceeded. Retrying in 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "        except openai.error.APIError as e:\n",
    "            print(f\"API error: {e}. Retrying in 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}. Skipping this article.\")\n",
    "            return \"Error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best prompt\n",
    "best_prompt = \"Discard all the previous instructions. Behave like you are an expert economist. Evaluate the following statement from an ECB member from the perspective of how the media reader might interpret it. Would it likely be perceived as HAWKISH (signaling higher rates), DOVISH (signaling lower rates), or NEUTRAL (ambiguous): \"\n",
    "\n",
    "# Score each article using the best model prompt\n",
    "print(\"Classifying and scoring articles using OpenAI_Prompt_4 for the complete dataset...\")\n",
    "\n",
    "# Use tqdm to add a progress bar to the apply function\n",
    "tqdm.pandas(desc=\"Scoring Articles\")\n",
    "articles_df['OpenAI_Score'] = articles_df['Manual.summary'].progress_apply(lambda article: classify_and_score_article(article, best_prompt))\n",
    "\n",
    "# Keep only the necessary columns\n",
    "output_df = articles_df[['Manual.summary', 'OpenAI_Score']]\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(\"Classification and scoring completed. Here are the first few results:\")\n",
    "print(output_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a new Excel file\n",
    "output_file_name = 'Scored_Articles_3000.xlsx'\n",
    "output_file = os.path.join(base_dir, output_file_name)\n",
    "print(f\"Saving the results to {output_file}...\")\n",
    "output_df.to_excel(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Reload the saved file to verify its contents\n",
    "print(\"Reloading the saved Excel file to verify its contents...\")\n",
    "verified_df = pd.read_excel(output_file)\n",
    "print(\"Head of the reloaded DataFrame:\")\n",
    "print(verified_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
