{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import os  # To access environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify OpenAI library version\n",
    "print(f\"OpenAI library version: {openai.__version__}\")  # Should print 0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Discard all the previous instructions. Behave like you are an expert sentence classifier. Classify the following statement from an ECB member into ‘HAWKISH’, ‘DOVISH’, or ‘NEUTRAL’. Label it ‘HAWKISH’ if it suggests tightening monetary policy, ‘DOVISH’ if it suggests easing monetary policy, or ‘NEUTRAL’ if the stance is ambiguous or not directly related to monetary policy: \",\n",
    "    \"Discard all the previous instructions. Behave like you are an expert sentence classifier. Assess the following statement from an ECB member in the context of maintaining price stability and promoting maximum employment. Determine whether it reflects a HAWKISH (tightening), DOVISH (easing), or NEUTRAL stance on monetary policy or is ambiguous: \",\n",
    "    \"Discard all the previous instructions. Behave like you are an expert sentence classifier. Analyze the following statement from an ECB member and identify the likely impact on interest rates, economic growth, and inflation. Based on your analysis, classify the sentence as HAWKISH, DOVISH, or NEUTRAL: \",\n",
    "    \"Discard all the previous instructions. Behave like you are an expert economist. Evaluate the following statement from an ECB member from the perspective of how the media reader might interpret it. Would it likely be perceived as HAWKISH (signaling higher rates), DOVISH (signaling lower rates), or NEUTRAL (ambiguous): \",\n",
    "    \"Discard all the previous instructions. Behave like you are an expert economist. Given current economic conditions, determine whether the following statement from an ECB member suggests a HAWKISH (concerned about inflation), DOVISH (concerned about growth), or NEUTRAL stance on monetary policy or ambiguous\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_csv('Random_Articles_to_Score_Complete_Converted.csv')\n",
    "print(\"Dataset loaded successfully.\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns and rename column as requested\n",
    "data.drop(columns=['Column1', 'Classification Joaquin', 'Classification Rui', 'Classification Ed'], inplace=True)\n",
    "data.rename(columns={'Average_Classification': 'Manual_Classification_Score'}, inplace=True)\n",
    "print(\"Columns dropped and renamed.\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_score_article(article, prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a financial expert.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt + article}\n",
    "                ],\n",
    "                max_tokens=150  # Adjust max_tokens if needed\n",
    "            )\n",
    "            result = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "            # Assign scores based on textual labels\n",
    "            if \"HAWKISH\" in result.upper():\n",
    "                score = 1\n",
    "            elif \"DOVISH\" in result.upper():\n",
    "                score = -1\n",
    "            elif \"NEUTRAL\" in result.upper():\n",
    "                score = 0\n",
    "            else:\n",
    "                raise ValueError(\"Incomplete response from API\")\n",
    "                \n",
    "            return score\n",
    "\n",
    "        except openai.error.RateLimitError:\n",
    "            print(\"Rate limit exceeded. Retrying in 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "        except openai.error.APIError as e:\n",
    "            print(f\"API error: {e}. Retrying in 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}. Skipping this article.\")\n",
    "            return \"Error\"\n",
    "\n",
    "# Initialize new columns for classifications from each prompt\n",
    "for i in range(1, 6):\n",
    "    data[f'OpenAI_Prompt_{i}'] = 0\n",
    "\n",
    "# Classify and score each article in the dataset\n",
    "print(\"Classifying and scoring articles...\")\n",
    "for i, article in enumerate(data['Manual.summary']):\n",
    "    for j, prompt in enumerate(prompts):\n",
    "        classification_score = classify_and_score_article(article, prompt)\n",
    "        data.at[i, f'OpenAI_Prompt_{j + 1}'] = classification_score\n",
    "    if (i + 1) % 10 == 0 or (i + 1) == len(data):\n",
    "        print(f\"Processed {i + 1} of {len(data)} articles.\")\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "print(\"Classification and scoring completed. Here are the first few results:\")\n",
    "print(data.head())\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_file = 'classified_articles_open_ai.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Print the DataFrame head of the results\n",
    "print(\"Here is the head of the resulting DataFrame:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the results\n",
    "results = {\n",
    "    'Prompt': [],\n",
    "    'Counts_Dovish': [],\n",
    "    'Counts_Neutral': [],\n",
    "    'Counts_Hawkish': [],\n",
    "    'Standard Deviation': [],\n",
    "    'Median': [],\n",
    "    'Average': []\n",
    "}\n",
    "\n",
    "# Calculate the counts, standard deviation, median, and average for each prompt\n",
    "for prompt in ['OpenAI_Prompt_1', 'OpenAI_Prompt_2', 'OpenAI_Prompt_3', 'OpenAI_Prompt_4', 'OpenAI_Prompt_5']:\n",
    "    # Ensure the scores are numeric\n",
    "    scores = pd.to_numeric(data[prompt], errors='coerce')\n",
    "    \n",
    "    # Calculate counts for each classification\n",
    "    counts = scores.value_counts().to_dict()\n",
    "    counts_dovish = counts.get(-1, 0)\n",
    "    counts_neutral = counts.get(0, 0)\n",
    "    counts_hawkish = counts.get(1, 0)\n",
    "    \n",
    "    # Calculate standard deviation, median, and average\n",
    "    std_dev = scores.std()\n",
    "    median = scores.median()\n",
    "    average = scores.mean()\n",
    "    \n",
    "    # Store the results\n",
    "    results['Prompt'].append(prompt)\n",
    "    results['Counts_Dovish'].append(counts_dovish)\n",
    "    results['Counts_Neutral'].append(counts_neutral)\n",
    "    results['Counts_Hawkish'].append(counts_hawkish)\n",
    "    results['Standard Deviation'].append(std_dev)\n",
    "    results['Median'].append(median)\n",
    "    results['Average'].append(average)\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Classification and scoring summary:\")\n",
    "results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a new CSV file\n",
    "output_file = 'classified_articles_open_ai_with_stats.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in Manual_Classifier_Statistics.csv and left join to results_df, then closest average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in the CSV file\n",
    "manual_articles_df = pd.read_csv('Manual_Classifier_Statistics.csv')\n",
    "\n",
    "manual_articles_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_articles_df = pd.read_csv('classified_articles_open_ai_with_stats.csv')\n",
    "open_ai_articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the average of the 'Average' column in manual_articles_df\n",
    "manual_average = manual_articles_df['Average'].mean()\n",
    "print(f\"Average of the 'Average' column in manual_articles_df: {manual_average}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Subtract this average from the 'Average' column in open_ai_articles_df\n",
    "open_ai_articles_df['Difference from Manual Average'] = open_ai_articles_df['Average'] - manual_average\n",
    "\n",
    "# Display the updated open_ai_articles_df\n",
    "print(\"Updated open_ai_articles_df with Difference from Manual Average:\")\n",
    "print(open_ai_articles_df)\n",
    "\n",
    "# Identify the prompt most similar to the manual classifiers based on the smallest absolute difference\n",
    "open_ai_articles_df['Absolute Difference'] = open_ai_articles_df['Difference from Manual Average'].abs()\n",
    "most_similar_prompt = open_ai_articles_df.loc[open_ai_articles_df['Absolute Difference'].idxmin()]\n",
    "\n",
    "print(\"Prompt most similar to manual classifiers:\")\n",
    "print(most_similar_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
