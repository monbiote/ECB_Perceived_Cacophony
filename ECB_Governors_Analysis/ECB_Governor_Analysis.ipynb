{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Enable progress_apply with progress bar in Jupyter environments\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim import models\n",
    "from scipy.sparse import csr_matrix\n",
    "import missingno as msno\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/ruimaciel/Desktop/Barcelona/Master_Thesis/ECB_Perceived_Cacophony/combined_updated.xlsx')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly.io as pio\n",
    "#pio.renderers.default = 'notebook'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcess Scraped Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped = pd.read_excel('/Users/ruimaciel/Desktop/Barcelona/Master_Thesis/ECB_Perceived_Cacophony/combined_updated.xlsx')\n",
    "df_scraped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where both 'Manual.summary' and 'Translated.text' are null or empty\n",
    "null_rows = df[(df['Manual.summary'].isnull() | df['Manual.summary'].eq('')) & \n",
    "               (df['Translated.text'].isnull() | df['Translated.text'].eq(''))]\n",
    "\n",
    "# Print the selected rows\n",
    "print(null_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where both 'Manual.summary' and 'Translated.text' are null or empty or contain '-'\n",
    "null_rows = df_scraped[(df_scraped['Manual.summary'].isnull() | df_scraped['Manual.summary'].eq('') | df_scraped['Manual.summary'].eq('-')) & \n",
    "                       (df_scraped['Translated.text'].isnull() | df_scraped['Translated.text'].eq(''))]\n",
    "\n",
    "# Count the number of rows where both 'Manual.summary' and 'Translated.text' are null or empty or contain '-'\n",
    "count_both_missing = null_rows.shape[0]\n",
    "\n",
    "# Calculate the total number of rows in the DataFrame\n",
    "total_rows = df_scraped.shape[0]\n",
    "\n",
    "# Calculate the percentage of rows where both 'Manual.summary' and 'Translated.text' are null or empty or contain '-'\n",
    "percentage_both_missing = (count_both_missing / total_rows) * 100\n",
    "\n",
    "# Print the number of rows and the percentage of total rows where both 'Manual.summary' and 'Translated.text' are null or empty or contain '-'\n",
    "print(f\"Number of rows where both 'Manual.summary' and 'Translated.text' are null, empty, or contain '-': {count_both_missing}\")\n",
    "print(f\"Percentage of total rows where both 'Manual.summary' and 'Translated.text' are null, empty, or contain '-': {percentage_both_missing:.2f}%\")\n",
    "\n",
    "# Print the selected rows\n",
    "print(\"\\nRows where both 'Manual.summary' and 'Translated.text' are null, empty, or contain '-':\")\n",
    "print(null_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Manual Summary and Scraped Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources if not already available\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Prepare the stopwords set once to avoid repeated loading\n",
    "initial_stop_words = stopwords.words('english')\n",
    "custom_stopwords = {\n",
    "    'npagina', 'nfoglio', 'nquotidiano', 'ndi', 'nla', 'ndie', 'nder', 'ne', 'nlos', 'nde', \n",
    "    'na', 'nto', 'np', 'nel', 'nque', 'nen', 'ndel', 'cm'\n",
    "}\n",
    "stop_words = set(initial_stop_words).union(custom_stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perform initial cleaning of the text by removing URLs, numbers,\n",
    "    non-alphabetic characters, and converting to lowercase.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove all standalone numbers\n",
    "    text = re.sub(r'\\bn[a-z]{2,}\\b', '', text)  # Aggressively remove words starting with 'n' followed by at least two letters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabet characters\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize(text, mode):\n",
    "    \"\"\"\n",
    "    Tokenizes the text based on the given mode.\n",
    "    Mode 0: Convert text to lowercase.\n",
    "    Mode 1: Apply stemming.\n",
    "    Mode 2: Apply lemmatization.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)  # Apply enhanced cleaning\n",
    "    words = word_tokenize(text)\n",
    "    if mode == 1:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    elif mode == 2:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_texts(data_frame, column_name, mode):\n",
    "    \"\"\"\n",
    "    Apply text preprocessing to a specified column in a DataFrame.\n",
    "    \"\"\"\n",
    "    # Fill NaN values with empty strings before processing\n",
    "    data_frame[column_name] = data_frame[column_name].fillna('')\n",
    "    # Use tqdm to display progress while applying preprocessing\n",
    "    tqdm.pandas(desc=\"Processing Texts\")\n",
    "    return data_frame[column_name].astype(str).progress_apply(lambda row: tokenize(row, mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mod = 2  # Set the preprocessing mode to 2 for lemmatizing\n",
    "\n",
    "# Reset the index of the DataFrame to avoid duplicate labels\n",
    "df_scraped.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Determine which column to preprocess for translated text\n",
    "df_scraped['text_to_preprocess'] = np.where(df_scraped['Translated.text'].fillna('').str.strip() == '',\n",
    "                                            df_scraped['Manual.summary'], \n",
    "                                            df_scraped['Translated.text'])\n",
    "\n",
    "# Apply preprocessing to the determined text\n",
    "df_scraped['translated_text_preproc'] = preprocess_texts(df_scraped, 'text_to_preprocess', mod)\n",
    "\n",
    "# Apply preprocessing to the Manual.summary column\n",
    "df_scraped['manual_summary_preproc'] = preprocess_texts(df_scraped, 'Manual.summary', mod)\n",
    "\n",
    "# Remove the auxiliary column if no longer needed\n",
    "df_scraped.drop(columns=['text_to_preprocess'], inplace=True)\n",
    "\n",
    "# Display the result to verify\n",
    "display(df_scraped[['Source', 'Manual.summary', 'Translated.text', 'Language', 'translated_text_preproc', 'manual_summary_preproc']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECB Governors Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECB_df = pd.read_csv('ecb_governing_council.csv')\n",
    "ECB_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate name variations\n",
    "def generate_name_variations(full_name, first_name, surname):\n",
    "    variations = [\n",
    "        full_name,\n",
    "        first_name,\n",
    "        surname,\n",
    "        f\"{first_name} {surname}\",\n",
    "        f\"{surname} {first_name}\",\n",
    "        full_name.replace(\" \", \"\"),\n",
    "        first_name.replace(\" \", \"\"),\n",
    "        surname.replace(\" \", \"\"),\n",
    "    ]\n",
    "    return list(set(variations))  # Ensure unique variations\n",
    "\n",
    "# Function to count occurrences of names\n",
    "def count_occurrences(text, names):\n",
    "    count = 0\n",
    "    for name in names:\n",
    "        count += len(re.findall(rf'\\b{name}\\b', text, flags=re.IGNORECASE))\n",
    "    return count\n",
    "\n",
    "# Extract names and countries from ECB_df\n",
    "full_names = ECB_df['Full Name'].tolist()\n",
    "first_names = ECB_df['First Name'].tolist()\n",
    "surnames = ECB_df['Surname'].tolist()\n",
    "countries = ECB_df['Country'].tolist()  # Update variable name to 'countries'\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Generate name variations for each governor\n",
    "name_variations_list = [\n",
    "    generate_name_variations(row['Full Name'], row['First Name'], row['Surname'])\n",
    "    for index, row in ECB_df.iterrows()\n",
    "]\n",
    "\n",
    "# Iterate through the governors and count occurrences in the preprocessed columns\n",
    "for index, row in ECB_df.iterrows():\n",
    "    full_name = row['Full Name']\n",
    "    first_name = row['First Name']\n",
    "    surname = row['Surname']\n",
    "    country = row['Country']  # Include the country\n",
    "\n",
    "    name_variations = generate_name_variations(full_name, first_name, surname)\n",
    "\n",
    "    full_name_count_translated = df_scraped['translated_text_preproc'].apply(lambda x: count_occurrences(x, name_variations)).sum()\n",
    "    first_name_count_translated = df_scraped['translated_text_preproc'].apply(lambda x: count_occurrences(x, [first_name])).sum()\n",
    "    surname_count_translated = df_scraped['translated_text_preproc'].apply(lambda x: count_occurrences(x, [surname])).sum()\n",
    "\n",
    "    full_name_count_manual = df_scraped['manual_summary_preproc'].apply(lambda x: count_occurrences(x, name_variations)).sum()\n",
    "    first_name_count_manual = df_scraped['manual_summary_preproc'].apply(lambda x: count_occurrences(x, [first_name])).sum()\n",
    "    surname_count_manual = df_scraped['manual_summary_preproc'].apply(lambda x: count_occurrences(x, [surname])).sum()\n",
    "\n",
    "    results.append({\n",
    "        'Governor': full_name,\n",
    "        'Country': country,  # Include the country in the results\n",
    "        'Full Name Count (Scraped Text)': full_name_count_translated,\n",
    "        'First Name Count (Scraped Text)': first_name_count_translated,\n",
    "        'Surname Count (Scraped Text)': surname_count_translated,\n",
    "        'Full Name Count (Manual Summary)': full_name_count_manual,\n",
    "        'First Name Count (Manual Summary)': first_name_count_manual,\n",
    "        'Surname Count (Manual Summary)': surname_count_manual\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "count_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the result to verify\n",
    "count_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate name variations\n",
    "def generate_name_variations(full_name, first_name, surname):\n",
    "    variations = [\n",
    "        full_name,\n",
    "        first_name,\n",
    "        surname,\n",
    "        f\"{first_name} {surname}\",\n",
    "        f\"{surname} {first_name}\",\n",
    "        full_name.replace(\" \", \"\"),\n",
    "        first_name.replace(\" \", \"\"),\n",
    "        surname.replace(\" \", \"\"),\n",
    "    ]\n",
    "    return list(set(variations))  # Ensure unique variations\n",
    "\n",
    "# Function to count occurrences of names\n",
    "def count_occurrences(text, names):\n",
    "    count = 0\n",
    "    for name in names:\n",
    "        count += len(re.findall(rf'\\b{name}\\b', text, flags=re.IGNORECASE))\n",
    "    return count\n",
    "\n",
    "# Extract names and countries from ECB_df\n",
    "full_names = ECB_df['Full Name'].tolist()\n",
    "first_names = ECB_df['First Name'].tolist()\n",
    "surnames = ECB_df['Surname'].tolist()\n",
    "countries = ECB_df['Country'].tolist()  # Update variable name to 'countries'\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Generate name variations for each governor\n",
    "name_variations_list = [\n",
    "    generate_name_variations(row['Full Name'], row['First Name'], row['Surname'])\n",
    "    for index, row in ECB_df.iterrows()\n",
    "]\n",
    "\n",
    "# Iterate through the governors and count occurrences in the preprocessed columns\n",
    "for index, row in ECB_df.iterrows():\n",
    "    full_name = row['Full Name']\n",
    "    first_name = row['First Name']\n",
    "    surname = row['Surname']\n",
    "    country = row['Country']  # Include the country\n",
    "\n",
    "    name_variations = generate_name_variations(full_name, first_name, surname)\n",
    "\n",
    "    full_name_count_translated = df_scraped['translated_text_preproc'].apply(lambda x: count_occurrences(x, name_variations)).sum()\n",
    "    first_name_count_translated = df_scraped['translated_text_preproc'].apply(lambda x: count_occurrences(x, [first_name])).sum()\n",
    "    surname_count_translated = df_scraped['translated_text_preproc'].apply(lambda x: count_occurrences(x, [surname])).sum()\n",
    "\n",
    "    full_name_count_manual = df_scraped['manual_summary_preproc'].apply(lambda x: count_occurrences(x, name_variations)).sum()\n",
    "    first_name_count_manual = df_scraped['manual_summary_preproc'].apply(lambda x: count_occurrences(x, [first_name])).sum()\n",
    "    surname_count_manual = df_scraped['manual_summary_preproc'].apply(lambda x: count_occurrences(x, [surname])).sum()\n",
    "\n",
    "    results.append({\n",
    "        'Governor': full_name,\n",
    "        'Country': country,  # Include the country in the results\n",
    "        'Full Name Count (Scraped Text)': full_name_count_translated,\n",
    "        'First Name Count (Scraped Text)': first_name_count_translated,\n",
    "        'Surname Count (Scraped Text)': surname_count_translated,\n",
    "        'Full Name Count (Manual Summary)': full_name_count_manual,\n",
    "        'First Name Count (Manual Summary)': first_name_count_manual,\n",
    "        'Surname Count (Manual Summary)': surname_count_manual\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "count_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the result to verify\n",
    "count_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df_scraped['Name_of_Speaker'] = None  # Initialize the column\n",
    "\n",
    "def check_and_return_full_name(text, name_variations, full_name):\n",
    "    # First, check for the specific case of Martins Kazaks\n",
    "    martins_kazaks_variations = ['mrti kazks', 'mrti', 'kazks']\n",
    "    for mk_name in martins_kazaks_variations:\n",
    "        if re.search(rf'\\b{mk_name}\\b', text, flags=re.IGNORECASE):\n",
    "            return 'Martins Kazaks'\n",
    "    # Then check other name variations\n",
    "    for name in name_variations:\n",
    "        if re.search(rf'\\b{name}\\b', text, flags=re.IGNORECASE):\n",
    "            return full_name\n",
    "    return None\n",
    "\n",
    "# Assume generate_name_variations is a function that takes full name, first name, and surname and returns a list of name variations\n",
    "# Iterate through each row in ECB_df to check matches in df_scraped\n",
    "for index, row in ECB_df.iterrows():\n",
    "    full_name = row['Full Name']\n",
    "    name_variations = generate_name_variations(full_name, row['First Name'], row['Surname'])\n",
    "\n",
    "    # Update df_scraped 'Name_of_Speaker' column where matches are found\n",
    "    df_scraped['Name_of_Speaker'] = df_scraped.apply(\n",
    "        lambda x: check_and_return_full_name(x['translated_text_preproc'], name_variations, full_name)\n",
    "        if x['Name_of_Speaker'] is None else x['Name_of_Speaker'], axis=1)\n",
    "\n",
    "    df_scraped['Name_of_Speaker'] = df_scraped.apply(\n",
    "        lambda x: check_and_return_full_name(x['manual_summary_preproc'], name_variations, full_name)\n",
    "        if x['Name_of_Speaker'] is None else x['Name_of_Speaker'], axis=1)\n",
    "\n",
    "# Print updated DataFrame\n",
    "print(df_scraped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_scraped is already loaded with data\n",
    "\n",
    "# Count occurrences of each unique value in 'Name_of_Speaker'\n",
    "name_counts = df_scraped['Name_of_Speaker'].value_counts()\n",
    "print(name_counts)\n",
    "\n",
    "\n",
    "# Plotting the counts\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size for better readability\n",
    "name_counts.plot(kind='bar', color='skyblue')  # Create a bar plot\n",
    "plt.title('Count of Each Speaker in Data')  # Title of the plot\n",
    "plt.xlabel('Name of Speaker')  # Label for the x-axis\n",
    "plt.ylabel('Counts')  # Label for the y-axis\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust subplots to give some padding\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_scraped.to_csv('df_scraped_with_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the non-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped_2=pd.read_csv('df_scraped_with_names.csv')\n",
    "df_scraped_2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the rows which Manual.Summary is Nan or it has less than 10 characters\n",
    "print(df_scraped_2[(df_scraped_2['manual_summary_preproc'].isnull()) & (df_scraped_2['Name_of_Speaker'].isnull())])\n",
    "# Count the rows which Manual.Summary is Nan or it has less than 10 characters\n",
    "print(df_scraped_2[(df_scraped_2['manual_summary_preproc'].isnull()) & (df_scraped_2['Name_of_Speaker'].isnull())].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe only with the manual_summary_preproc is null\n",
    "df_scraped_null = df_scraped_2[(df_scraped_2['manual_summary_preproc'].isnull()) & (df_scraped_2['Name_of_Speaker'].isnull())]\n",
    "df_scraped_null.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scraped_null.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the rows where both 'manual_summary_preproc' and 'Name_of_Speaker' are NaN\n",
    "print(df_scraped_2[(df_scraped_2['manual_summary_preproc'].isnull()) & (df_scraped_2['Name_of_Speaker'].isnull())])\n",
    "\n",
    "# Print the total number of rows before removing any\n",
    "print(\"Total rows before removing any:\", df_scraped_2.shape[0])\n",
    "\n",
    "# Remove rows where 'manual_summary_preproc' is null or 'Name_of_Speaker' is null\n",
    "df_scraped_2 = df_scraped_2[~(df_scraped_2['manual_summary_preproc'].isnull() & df_scraped_2['Name_of_Speaker'].isnull())]\n",
    "\n",
    "# Print the total number of rows after removing rows where 'manual_summary_preproc' or 'Name_of_Speaker' is null\n",
    "print(\"Total rows after removing rows with any NaNs in 'manual_summary_preproc' or 'Name_of_Speaker':\", df_scraped_2.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the percentage of null_rows in the 'Name_of_Speaker' column\n",
    "null_rows = df_scraped[df_scraped['Name_of_Speaker'].isnull()]\n",
    "count_both_missing = null_rows.shape[0]\n",
    "total_rows = df_scraped.shape[0]\n",
    "percentage_both_missing_1 = (count_both_missing / total_rows) * 100\n",
    "\n",
    "print(f\"Number of rows where 'Name_of_Speaker' is null in df_shaped: {count_both_missing}\")\n",
    "print (f\"Percentage of total rows where 'Name_of_Speaker' is null in df_shaped: {percentage_both_missing_1:.2f}%\")\n",
    "\n",
    "#Check the percentage of null_rows in the 'Name_of_Speaker' column\n",
    "null_rows = df_scraped_2[df_scraped_2['Name_of_Speaker'].isnull()]\n",
    "count_both_missing = null_rows.shape[0]\n",
    "total_rows = df_scraped_2.shape[0]\n",
    "percentage_both_missing_2 = (count_both_missing / total_rows) * 100\n",
    "\n",
    "print(f\"Number of rows where 'Name_of_Speaker' is null in df_shaped_2: {count_both_missing}\")\n",
    "print (f\"Percentage of total rows where 'Name_of_Speaker' is null in df_shaped_2: {percentage_both_missing_2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of rows where 'Name_of_Speaker' is null\n",
    "print(f\"Percentage of total rows where 'Name_of_Speaker' is null: {percentage_both_missing:.2f}%\")\n",
    "print(f'{count_both_missing} rows where Name_of_Speaker is null')\n",
    "# Print the total number of rows\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Check and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for wide windows\n",
    "pd.set_option('display.max_colwidth', None)  # Use None to show full content without truncation\n",
    "pd.set_option('display.width', 1000)  # Adjust the number to fit your display or preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped_2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count the number of rows without a Name_of_Speaker\n",
    "missing_names_count = df_scraped_2['Name_of_Speaker'].isna().sum()\n",
    "print(f\"Number of rows without Name_of_Speaker: {missing_names_count}\")\n",
    "\n",
    "# Step 2: Remove rows where Name_of_Speaker is missing\n",
    "df_cleaned = df_scraped_2.dropna(subset=['Name_of_Speaker'])\n",
    "\n",
    "# Optionally print the shape to confirm rows are removed\n",
    "print(f\"New DataFrame shape after removing rows without Name_of_Speaker: {df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count the number of rows without a Manual.summary\n",
    "missing_summaries_count = df_scraped_2['Manual.summary'].isna().sum()\n",
    "print(f\"Number of rows without Manual.summary: {missing_summaries_count}\")\n",
    "\n",
    "# Step 2: Remove rows where Manual.summary is missing\n",
    "df_cleaned = df_scraped_2.dropna(subset=['Manual.summary'])\n",
    "\n",
    "# Optionally print the shape to confirm rows are removed\n",
    "print(f\"New DataFrame shape after removing rows without Manual.summary: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for duplicates\n",
    "columns_to_check = [\n",
    "    'Date', 'Source', 'Headline', 'Translated.headline', 'Manual.summary', \n",
    "    'Original.article.url', 'Translated.text', 'Media.type', 'Speaker', \n",
    "    'Reach', 'Language', 'Country.Code', 'Sitename', 'Tags'\n",
    "]\n",
    "\n",
    "# Identifying duplicates based on the list of columns\n",
    "duplicates = df_scraped_2[df_scraped_2.duplicated(columns_to_check, keep=False)]\n",
    "\n",
    "# Sorting the duplicates to group identical entries together\n",
    "duplicates_sorted = duplicates.sort_values(by=columns_to_check)\n",
    "\n",
    "print(f\"Number of duplicate entries: {duplicates_sorted.shape[0]}\")\n",
    "duplicates_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on the list of columns\n",
    "df_cleaned = df_scraped_2.drop_duplicates(subset=columns_to_check, keep='first')\n",
    "\n",
    "# Print the shape to confirm rows are removed\n",
    "print(f\"DataFrame shape after removing duplicates: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of manual summaries with less than 30 characters\n",
    "count_less_than_30 = (df_scraped_2['Manual.summary'].str.len() <30).sum()\n",
    "\n",
    "print(f\"Number of manual summaries with less than 30 characters: {count_less_than_30}\")\n",
    "\n",
    "# Filter and print only the 'Manual.summary' column where summaries have less than 30 characters\n",
    "summaries_less_than_30 = df_scraped_2.loc[df_scraped_2['Manual.summary'].str.len() < 30, 'Manual.summary']\n",
    "print(summaries_less_than_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to remove rows where the 'Manual.summary' column has summaries less than 30 characters\n",
    "df_cleaned = df_scraped_2[df_scraped_2['Manual.summary'].str.len() >= 30]\n",
    "\n",
    "# Optionally print the cleaned DataFrame to confirm rows are removed\n",
    "print(f\"DataFrame shape after removing short summaries: {df_cleaned.shape}\")\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking that everythins is similar afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Count the occurrences for each language\n",
    "language_counts = df_cleaned['Language'].value_counts().reset_index()\n",
    "language_counts.columns = ['Language', 'Count']\n",
    "\n",
    "# Count the occurrences for each country code\n",
    "country_code_counts = df['Country.Code'].value_counts().reset_index()\n",
    "country_code_counts.columns = ['Country Code', 'Count']\n",
    "\n",
    "# Create a figure with one row and two columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Article Counts by Language', 'Article Counts by Country Code'))\n",
    "\n",
    "# Plotting the 'Language' bar plot with Plotly in the first column\n",
    "fig.add_trace(\n",
    "    go.Bar(x=language_counts['Language'], y=language_counts['Count'], \n",
    "           marker=dict(color=px.colors.sequential.Blues[-2])),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Plotting the 'Country Code' bar plot with Plotly in the second column\n",
    "fig.add_trace(\n",
    "    go.Bar(x=country_code_counts['Country Code'], y=country_code_counts['Count'], \n",
    "           marker=dict(color=px.colors.sequential.Reds[-2])),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update xaxis properties for both plots for better readability\n",
    "fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "\n",
    "# Update layout to adjust spacing and add some styling\n",
    "fig.update_layout(\n",
    "    height=500,  # Adjusted for better proportion in side-by-side layout\n",
    "    width=1500,  # Adjust width to accommodate side-by-side layout\n",
    "    showlegend=False,\n",
    "    title_text=\"Article Counts by Language and Country Code\",\n",
    "    title_x=0.5  # Centering the title\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_scraped is already loaded with data\n",
    "\n",
    "# Count occurrences of each unique value in 'Name_of_Speaker'\n",
    "name_counts = df_cleaned['Name_of_Speaker'].value_counts()\n",
    "print(name_counts)\n",
    "\n",
    "\n",
    "# Plotting the counts\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size for better readability\n",
    "name_counts.plot(kind='bar', color='skyblue')  # Create a bar plot\n",
    "plt.title('Count of Each Speaker in Data')  # Title of the plot\n",
    "plt.xlabel('Name of Speaker')  # Label for the x-axis\n",
    "plt.ylabel('Counts')  # Label for the y-axis\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust subplots to give some padding\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each media type\n",
    "media_type_counts = df_cleaned['Media.type'].value_counts()\n",
    "\n",
    "# Create a DataFrame from the media type counts\n",
    "media_type_df = media_type_counts.reset_index()\n",
    "media_type_df.columns = ['Media.type', 'Count']  # Rename columns for clarity\n",
    "\n",
    "# Show the top 10 most frequent media types\n",
    "print(media_type_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of articles per Sitename\n",
    "site_article_counts = df_cleaned['Sitename'].value_counts()\n",
    "\n",
    "site_article_counts.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cleaned.to_csv('df_cleaned_final_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOUBLE CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.read_csv('/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/df_cleaned_final_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values on Name_of_Speaker\n",
    "missing_names_count = df_final['Name_of_Speaker'].isna().sum()\n",
    "print(f\"Number of rows without Name_of_Speaker: {missing_names_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing values on Name_of_Speaker\n",
    "df_final = df_final.dropna(subset=['Name_of_Speaker'])\n",
    "\n",
    "#Print of shape of the dataframe after removing missing values on Name_of_Speaker\n",
    "print(f\"New DataFrame shape after removing rows without Name_of_Speaker: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values on Manual.summary\n",
    "missing_summaries_count = df_final['Manual.summary'].isna().sum()\n",
    "print(f\"Number of rows without Manual.summary: {missing_summaries_count}\")\n",
    "\n",
    "# List of columns to check for duplicates\n",
    "columns_to_check = [\n",
    "    'Date', 'Source', 'Headline', 'Translated.headline', 'Manual.summary', \n",
    "    'Original.article.url', 'Translated.text', 'Media.type', 'Speaker', \n",
    "    'Reach', 'Language', 'Country.Code', 'Sitename', 'Tags'\n",
    "]\n",
    "\n",
    "# Identifying duplicates based on the list of columns\n",
    "duplicates = df_final[df_final.duplicated(columns_to_check, keep=False)]\n",
    "\n",
    "# Sorting the duplicates to group identical entries together\n",
    "duplicates_sorted = duplicates.sort_values(by=columns_to_check)\n",
    "\n",
    "print(f\"Number of duplicate entries: {duplicates_sorted.shape[0]}\")\n",
    "duplicates_sorted\n",
    "\n",
    "# Remove duplicates based on the list of columns\n",
    "df_final = df_final.drop_duplicates(subset=columns_to_check, keep='first')\n",
    "# Print the shape to confirm rows are removed\n",
    "print(f\"DataFrame shape after removing duplicates: {df_final.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/df_cleaned_final_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Governors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "#Create the bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=count_df['Governor'],\n",
    "    y=count_df['Full Name Count (Scraped Text)'],\n",
    "    name='Full Name Count (Scraped Text)'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=count_df['Governor'],\n",
    "    y=count_df['Full Name Count (Manual Summary)'],\n",
    "    name='Full Name Count (Manual Summary)'\n",
    "))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title='Comparison of Full Name Counts',\n",
    "    xaxis_title='Governor',\n",
    "    yaxis_title='Count',\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to check for a match with variations\n",
    "def has_match(text, name_variations):\n",
    "    for name in name_variations:\n",
    "        if re.search(rf'\\b{name}\\b', text, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Create a function to generate name variations\n",
    "def generate_name_variations(full_name, first_name, surname):\n",
    "    variations = [\n",
    "        full_name,\n",
    "        first_name,\n",
    "        surname,\n",
    "        f\"{first_name} {surname}\",\n",
    "        f\"{surname} {first_name}\",\n",
    "        full_name.replace(\" \", \"\"),\n",
    "        first_name.replace(\" \", \"\"),\n",
    "        surname.replace(\" \", \"\"),\n",
    "    ]\n",
    "    return list(set(variations))  # Ensure unique variations\n",
    "\n",
    "# List of full names and their variations\n",
    "name_variations_list = [\n",
    "    generate_name_variations(row['Full Name'], row['First Name'], row['Surname'])\n",
    "    for index, row in ECB_df.iterrows()\n",
    "]\n",
    "\n",
    "# Check for matches in the preprocessed columns and count articles with at least one match\n",
    "total_articles = len(df_scraped)\n",
    "full_name_matches_translated = df_scraped['translated_text_preproc'].apply(\n",
    "    lambda x: any(has_match(x, name_variations) for name_variations in name_variations_list)\n",
    ").sum()\n",
    "full_name_matches_manual = df_scraped['manual_summary_preproc'].apply(\n",
    "    lambda x: any(has_match(x, name_variations) for name_variations in name_variations_list)\n",
    ").sum()\n",
    "\n",
    "# Calculate the proportion of articles with full name matches\n",
    "full_name_proportion_translated = full_name_matches_translated / total_articles\n",
    "full_name_proportion_manual = full_name_matches_manual / total_articles\n",
    "\n",
    "print(f\"Full Name Match Statistics:\")\n",
    "print(f\"Total articles: {total_articles}\")\n",
    "print(f\"Articles with full name match (Translated Text): {full_name_matches_translated} ({full_name_proportion_translated:.2%})\")\n",
    "print(f\"Articles with full name match (Manual Summary): {full_name_matches_manual} ({full_name_proportion_manual:.2%})\")\n",
    "\n",
    "# Filter articles without full name matches in the manual summary column and ignore blank entries\n",
    "articles_without_full_name_match_manual = df_scraped[df_scraped['manual_summary_preproc'].apply(\n",
    "    lambda x: not any(has_match(x, name_variations) for name_variations in name_variations_list) and x.strip() != ''\n",
    ")]\n",
    "\n",
    "# Randomly select 10 examples of such articles\n",
    "examples_without_full_name_match_manual = articles_without_full_name_match_manual.sample(n=10, random_state=1)\n",
    "\n",
    "# Display the examples with full content in a more readable format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"\\nExamples of articles without full name match (Manual Summary):\")\n",
    "for i, row in examples_without_full_name_match_manual.iterrows():\n",
    "    print(f\"Article {i+1}:\\n{row['manual_summary_preproc']}\\n{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for a match\n",
    "def has_any_match(text, full_names, first_names, surnames):\n",
    "    for name in full_names + first_names + surnames:\n",
    "        if re.search(rf'\\b{name}\\b', text, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Lists of names\n",
    "full_names = ECB_df['Full Name'].tolist()\n",
    "first_names = ECB_df['First Name'].tolist()\n",
    "surnames = ECB_df['Surname'].tolist()\n",
    "\n",
    "# Check for matches in the preprocessed columns and count articles with at least one match\n",
    "total_articles = len(df_scraped)\n",
    "any_name_matches_translated = df_scraped['translated_text_preproc'].apply(\n",
    "    lambda x: has_any_match(x, full_names, first_names, surnames)).sum()\n",
    "any_name_matches_manual = df_scraped['manual_summary_preproc'].apply(\n",
    "    lambda x: has_any_match(x, full_names, first_names, surnames)).sum()\n",
    "\n",
    "# Calculate the proportion of articles with any name matches\n",
    "any_name_proportion_translated = any_name_matches_translated / total_articles\n",
    "any_name_proportion_manual = any_name_matches_manual / total_articles\n",
    "\n",
    "print(f\"\\nFull Names + First Names + Surnames Match Statistics:\")\n",
    "print(f\"Total articles: {total_articles}\")\n",
    "print(f\"Articles with any name match (Translated Text): {any_name_matches_translated} ({any_name_proportion_translated:.2%})\")\n",
    "print(f\"Articles with any name match (Manual Summary): {any_name_matches_manual} ({any_name_proportion_manual:.2%})\")\n",
    "\n",
    "# Filter articles without any name matches in the manual summary column and ignore blank entries\n",
    "articles_without_any_name_match_manual = df_scraped[df_scraped['manual_summary_preproc'].apply(\n",
    "    lambda x: not has_any_match(x, full_names, first_names, surnames) and x.strip() != ''\n",
    ")]\n",
    "\n",
    "# Randomly select 10 examples of such articles\n",
    "examples_without_any_name_match_manual = articles_without_any_name_match_manual.sample(n=10, random_state=1)\n",
    "\n",
    "# Display the examples with full content in a more readable format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"\\nExamples of articles without any name match (Manual Summary):\")\n",
    "for i, row in examples_without_any_name_match_manual.iterrows():\n",
    "    print(f\"Article {i+1}:\\n{row['manual_summary_preproc']}\\n{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECB_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "articles_without_any_name_match_manual.to_csv('articles_without_any_name_match.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Analysis Hawkish & Dovish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the spaCy model (run this command only once)\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define expanded dovish and hawkish words\n",
    "dovish_words = [\n",
    "    \"accommodative\", \"easing\", \"expansionary\", \"stimulus\", \"lower rates\", \"dovish\", \"supportive\", \"lowering rates\",\n",
    "    \"quantitative easing\", \"QE\", \"liquidity\", \"easy money\", \"softening\", \"growth\", \"recovery\", \"reduce\", \"lowering\",\n",
    "    \"decrease\", \"supportive measures\", \"boost\", \"economic support\", \"fiscal support\", \"monetary support\", \"rate cuts\",\n",
    "    \"rate reduction\", \"interest rate cut\", \"low interest\", \"maintaining low rates\", \"deflationary\", \"deflation\", \"aid\",\n",
    "    \"bailout\", \"funding\", \"cash flow\", \"intervention\", \"credit easing\", \"backstop\"\n",
    "]\n",
    "\n",
    "hawkish_words = [\n",
    "    \"tightening\", \"contractionary\", \"restrictive\", \"hiking\", \"raising rates\", \"hawkish\", \"higher rates\", \"inflationary\",\n",
    "    \"quantitative tightening\", \"QT\", \"interest rate hike\", \"hard money\", \"hardening\", \"inflation\", \"price stability\",\n",
    "    \"curbing\", \"containing inflation\", \"reducing inflation\", \"anti-inflationary\", \"overheating\", \"fiscal discipline\",\n",
    "    \"monetary tightening\", \"rate hikes\", \"rate increase\", \"interest rate rise\", \"high interest\", \"reducing stimulus\",\n",
    "    \"economic restraint\", \"budget cuts\", \"austerity\", \"monetary discipline\", \"tapering\", \"withdrawal of stimulus\",\n",
    "    \"debt control\", \"fiscal consolidation\", \"financial tightening\"\n",
    "]\n",
    "\n",
    "# Custom stop words list\n",
    "custom_stop_words = list(nlp.Defaults.stop_words.union({\n",
    "    \"rate\", \"ecb\", \"bank\", \"central\", \"euro\", \"said\", \"president\", \"increase\", \"inflation\", \"lagarde\",\n",
    "    \"interest\", \"policy\", \"monetary\", \"market\", \"economy\", \"price\", \"point\", \"year\", \"high\", \"area\", \"christine\", \"mr\", \"de\", \"hike\",\n",
    "    \"percent\", \"billion\", \"million\", \"trillion\", \"europe\", \"union\", \"currency\", \"eurosystem\", \"system\", \"committee\",\n",
    "    \"board\", \"member\", \"members\", \"meeting\", \"conference\", \"discussion\", \"report\", \"statement\", \"notes\", \"speech\",\n",
    "    \"publication\", \"data\", \"figures\", \"information\", \"details\", \"analysis\", \"comments\", \"comment\", \"commentary\", \n",
    "    \"remarks\", \"outlook\", \"forecast\", \"projections\", \"expectations\", \"view\", \"views\", \"opinion\", \"opinions\", \"perspective\",\n",
    "    \"perspectives\", \"standpoint\", \"stance\", \"position\", \"positions\", \"policy\", \"policies\", \"approach\", \"approaches\", \"strategy\",\n",
    "    \"strategies\", \"framework\", \"frameworks\", \"program\", \"programs\", \"measures\", \"measure\", \"tools\", \"tool\", \"instrument\",\n",
    "    \"instruments\", \"mechanism\", \"mechanisms\", \"method\", \"methods\", \"procedure\", \"procedures\", \"process\", \"processes\",\n",
    "    \"implementation\", \"practice\", \"practices\", \"execution\", \"operation\", \"operations\", \"activity\", \"activities\", \n",
    "    \"function\", \"functions\", \"role\", \"roles\", \"task\", \"tasks\", \"responsibility\", \"responsibilities\", \"duty\", \"duties\",\n",
    "    \"obligation\", \"obligations\", \"commitment\", \"commitments\", \"engagement\", \"engagements\", \"undertaking\", \"undertakings\", \"according\", \"time\", \"according\", \"level\", \"council\", \"financial\", \"expected\", \"european\"\n",
    "}))\n",
    "\n",
    "# Function to extract potential sentiment words using spaCy with progress bar\n",
    "def extract_potential_words(texts, existing_words, label):\n",
    "    potential_words = []\n",
    "    for text in tqdm(texts, desc=f\"Processing texts with spaCy for {label}\"):\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if token.is_alpha and token.lemma_ not in existing_words and token.lemma_ not in custom_stop_words and token.pos_ in ['ADJ', 'NOUN', 'VERB']:\n",
    "                potential_words.append(token.lemma_)\n",
    "    return Counter(potential_words)\n",
    "\n",
    "# Combine both preprocessed columns\n",
    "all_texts = df_scraped['translated_text_preproc'].tolist() + df_scraped['manual_summary_preproc'].tolist()\n",
    "\n",
    "# Extract potential dovish and hawkish words using spaCy\n",
    "print(\"Extracting potential dovish words using spaCy...\")\n",
    "dovish_potential_spacy = extract_potential_words(all_texts, dovish_words + hawkish_words, \"dovish\")\n",
    "\n",
    "print(\"Extracting potential hawkish words using spaCy...\")\n",
    "hawkish_potential_spacy = extract_potential_words(all_texts, hawkish_words + dovish_words, \"hawkish\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with custom stop words\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words, max_features=1000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit and transform the text data with progress bar\n",
    "print(\"Calculating TF-IDF scores...\")\n",
    "tfidf_matrix = vectorizer.fit_transform(tqdm(all_texts, desc=\"TF-IDF transformation\"))\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with TF-IDF scores\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Calculate mean TF-IDF scores for each word\n",
    "mean_tfidf_scores = tfidf_df.mean()\n",
    "\n",
    "# Filter potential words based on their presence in TF-IDF feature names\n",
    "filtered_dovish_words = [word for word, count in dovish_potential_spacy.items() if word in feature_names]\n",
    "filtered_hawkish_words = [word for word, count in hawkish_potential_spacy.items() if word in feature_names]\n",
    "\n",
    "# Get top N TF-IDF words\n",
    "top_n = 15  # Update to get top 15 words\n",
    "print(\"Selecting top dovish words based on TF-IDF scores...\")\n",
    "top_dovish_tfidf = mean_tfidf_scores[filtered_dovish_words].sort_values(ascending=False).head(top_n)\n",
    "\n",
    "print(\"Selecting top hawkish words based on TF-IDF scores...\")\n",
    "top_hawkish_tfidf = mean_tfidf_scores[filtered_hawkish_words].sort_values(ascending=False).head(top_n)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top Dovish Words (with TF-IDF):\", top_dovish_tfidf)\n",
    "print(\"\\nTop Hawkish Words (with TF-IDF):\", top_hawkish_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we link the sentiment analysis to each governor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count sentiment words\n",
    "def count_sentiment_words(text, sentiment_words):\n",
    "    doc = nlp(text)\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.lemma_ in sentiment_words:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Adding dovish and hawkish counts to count_df\n",
    "for index, row in count_df.iterrows():\n",
    "    full_name = row['Governor']\n",
    "    # Filter texts containing the governor's name\n",
    "    governor_texts_translated = df_scraped[df_scraped['translated_text_preproc'].str.contains(full_name, case=False, na=False)]\n",
    "    governor_texts_manual = df_scraped[df_scraped['manual_summary_preproc'].str.contains(full_name, case=False, na=False)]\n",
    "\n",
    "    # Count dovish and hawkish words in the filtered texts\n",
    "    dovish_count_translated = governor_texts_translated['translated_text_preproc'].progress_apply(lambda x: count_sentiment_words(x, dovish_words)).sum()\n",
    "    hawkish_count_translated = governor_texts_translated['translated_text_preproc'].progress_apply(lambda x: count_sentiment_words(x, hawkish_words)).sum()\n",
    "    \n",
    "    dovish_count_manual = governor_texts_manual['manual_summary_preproc'].progress_apply(lambda x: count_sentiment_words(x, dovish_words)).sum()\n",
    "    hawkish_count_manual = governor_texts_manual['manual_summary_preproc'].progress_apply(lambda x: count_sentiment_words(x, hawkish_words)).sum()\n",
    "    \n",
    "    count_df.at[index, 'Dovish Count (Translated)'] = dovish_count_translated\n",
    "    count_df.at[index, 'Hawkish Count (Translated)'] = hawkish_count_translated\n",
    "    count_df.at[index, 'Dovish Count (Manual)'] = dovish_count_manual\n",
    "    count_df.at[index, 'Hawkish Count (Manual)'] = hawkish_count_manual\n",
    "\n",
    "# Calculate total sentiment-related words for each governor\n",
    "count_df['Total Dovish'] = count_df['Dovish Count (Translated)'] + count_df['Dovish Count (Manual)']\n",
    "count_df['Total Hawkish'] = count_df['Hawkish Count (Translated)'] + count_df['Hawkish Count (Manual)']\n",
    "count_df['Total Sentiment'] = count_df['Total Dovish'] + count_df['Total Hawkish']\n",
    "\n",
    "# Calculate the proportion of dovish and hawkish words for each governor\n",
    "count_df['Dovish Proportion'] = count_df['Total Dovish'] / count_df['Total Sentiment']\n",
    "count_df['Hawkish Proportion'] = count_df['Total Hawkish'] / count_df['Total Sentiment']\n",
    "\n",
    "# Calculate the hawkish-to-dovish ratio\n",
    "count_df['Hawkish/Dovish Ratio'] = count_df['Hawkish Proportion'] / count_df['Dovish Proportion']\n",
    "\n",
    "# Normalize the hawkish and dovish proportions for comparison\n",
    "count_df['Dovish Score (Normalized)'] = (count_df['Dovish Proportion'] - count_df['Dovish Proportion'].min()) / (count_df['Dovish Proportion'].max() - count_df['Dovish Proportion'].min())\n",
    "count_df['Hawkish Score (Normalized)'] = (count_df['Hawkish Proportion'] - count_df['Hawkish Proportion'].min()) / (count_df['Hawkish Proportion'].max() - count_df['Hawkish Proportion'].min())\n",
    "\n",
    "# Sort by Hawkish/Dovish Ratio in descending order\n",
    "count_df_sorted = count_df.sort_values(by='Hawkish/Dovish Ratio', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(count_df_sorted[['Governor', 'Country', 'Dovish Proportion', 'Hawkish Proportion', 'Hawkish/Dovish Ratio', 'Dovish Score (Normalized)', 'Hawkish Score (Normalized)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df_sorted.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loughran McDonald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load Loughran-McDonald Master Dictionary\n",
    "file_path = 'Loughran-McDonald_MasterDictionary_1993-2023.csv'\n",
    "master_dictionary_df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract sentiment words from the Loughran-McDonald dictionary\n",
    "negative_words = set(master_dictionary_df[master_dictionary_df['Negative'] > 0]['Word'].str.lower())\n",
    "positive_words = set(master_dictionary_df[master_dictionary_df['Positive'] > 0]['Word'].str.lower())\n",
    "uncertainty_words = set(master_dictionary_df[master_dictionary_df['Uncertainty'] > 0]['Word'].str.lower())\n",
    "litigious_words = set(master_dictionary_df[master_dictionary_df['Litigious'] > 0]['Word'].str.lower())\n",
    "constraining_words = set(master_dictionary_df[master_dictionary_df['Constraining'] > 0]['Word'].str.lower())\n",
    "\n",
    "# Function to count sentiment words\n",
    "def count_sentiment_words(text, sentiment_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    count = sum(1 for word in words if word in sentiment_words)\n",
    "    return count\n",
    "\n",
    "# Adding Loughran-McDonald sentiment counts to count_df\n",
    "for index, row in count_df.iterrows():\n",
    "    full_name = row['Governor']\n",
    "    # Filter texts containing the governor's name\n",
    "    governor_texts_translated = df_scraped[df_scraped['translated_text_preproc'].str.contains(full_name, case=False, na=False)]\n",
    "    governor_texts_manual = df_scraped[df_scraped['manual_summary_preproc'].str.contains(full_name, case=False, na=False)]\n",
    "    \n",
    "    # Concatenate all texts for each governor\n",
    "    all_texts_translated = \" \".join(governor_texts_translated['translated_text_preproc'])\n",
    "    all_texts_manual = \" \".join(governor_texts_manual['manual_summary_preproc'])\n",
    "    \n",
    "    # Count sentiment words in the concatenated texts\n",
    "    count_df.at[index, 'LM_Negative_Count'] = count_sentiment_words(all_texts_translated, negative_words) + count_sentiment_words(all_texts_manual, negative_words)\n",
    "    count_df.at[index, 'LM_Positive_Count'] = count_sentiment_words(all_texts_translated, positive_words) + count_sentiment_words(all_texts_manual, positive_words)\n",
    "    count_df.at[index, 'LM_Uncertainty_Count'] = count_sentiment_words(all_texts_translated, uncertainty_words) + count_sentiment_words(all_texts_manual, uncertainty_words)\n",
    "    count_df.at[index, 'LM_Litigious_Count'] = count_sentiment_words(all_texts_translated, litigious_words) + count_sentiment_words(all_texts_manual, litigious_words)\n",
    "    count_df.at[index, 'LM_Constraining_Count'] = count_sentiment_words(all_texts_translated, constraining_words) + count_sentiment_words(all_texts_manual, constraining_words)\n",
    "\n",
    "# Calculate total sentiment-related words for each governor\n",
    "count_df['Total_LM_Sentiment'] = (count_df['LM_Negative_Count'] + count_df['LM_Positive_Count'] +\n",
    "                                  count_df['LM_Uncertainty_Count'] + count_df['LM_Litigious_Count'] +\n",
    "                                  count_df['LM_Constraining_Count'])\n",
    "\n",
    "# Calculate the proportion of each sentiment type for each governor\n",
    "count_df['LM_Positive_Proportion'] = count_df['LM_Positive_Count'] / count_df['Total_LM_Sentiment']\n",
    "count_df['LM_Negative_Proportion'] = count_df['LM_Negative_Count'] / count_df['Total_LM_Sentiment']\n",
    "count_df['LM_Uncertainty_Proportion'] = count_df['LM_Uncertainty_Count'] / count_df['Total_LM_Sentiment']\n",
    "count_df['LM_Litigious_Proportion'] = count_df['LM_Litigious_Count'] / count_df['Total_LM_Sentiment']\n",
    "count_df['LM_Constraining_Proportion'] = count_df['LM_Constraining_Count'] / count_df['Total_LM_Sentiment']\n",
    "\n",
    "# Sort by LM Positive Proportion in descending order, then by LM Negative Proportion in ascending order\n",
    "count_df_sorted = count_df.sort_values(by=['LM_Positive_Proportion', 'LM_Negative_Proportion'], ascending=[False, True])\n",
    "\n",
    "# Display the relevant columns\n",
    "print(count_df_sorted[['Governor', 'Country', 'LM_Positive_Count', 'LM_Negative_Count', 'LM_Uncertainty_Count', 'LM_Litigious_Count', 'LM_Constraining_Count', 'LM_Positive_Proportion', 'LM_Negative_Proportion', 'LM_Uncertainty_Proportion', 'LM_Litigious_Proportion', 'LM_Constraining_Proportion']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df_sorted.head(30)[['Governor', 'Country', 'LM_Positive_Count', 'LM_Negative_Count', 'LM_Uncertainty_Count', 'LM_Litigious_Count', 'LM_Constraining_Count', 'LM_Positive_Proportion', 'LM_Negative_Proportion', 'LM_Uncertainty_Proportion', 'LM_Litigious_Proportion', 'LM_Constraining_Proportion']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
