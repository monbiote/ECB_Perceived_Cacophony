{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps, i thought we did something more complex for intro but this is what i can share that is applicable to our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see articles that are in English\n",
    "\n",
    "non_en_count = 0\n",
    "for index, row in ec_data.iterrows():\n",
    "    lang = detect(row['Text'])\n",
    "    if lang != 'en':  # 'en' represents English\n",
    "        non_en_count +=1\n",
    "print(f'Number of non-English articles: {non_en_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - lowercase, 1 - stemming, 2 - lemmatizing \n",
    "mod=2\n",
    "\n",
    "# Pre-process texts\n",
    "text_preproc = (\n",
    "    ec_data.Text\n",
    "    .astype(str)\n",
    "    .progress_apply(lambda row: tokenize(row, mod))\n",
    ")\n",
    "\n",
    "ec_data[\"text_preproc\"]=text_preproc\n",
    "\n",
    "print(\"Done with text!\")\n",
    "\n",
    "# Pre-process titles\n",
    "tit_preproc = (\n",
    "    ec_data.Title\n",
    "    .astype(str)\n",
    "    .progress_apply(lambda row: tokenize(row, mod))\n",
    ")\n",
    "\n",
    "ec_data[\"titles_preproc\"]=tit_preproc\n",
    "\n",
    "print(\"Done with titles!\")\n",
    "ec_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "wh_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data analsysis suggestion, this basically runs cv, td-idf and lda for the parameters we choose and prints every output to a csv. this is important to then select the parametrs for min df and max df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_text_data(data, min_df, max_df):\n",
    "    # Previous setup for CountVectorizer and TfidfVectorizer\n",
    "    \n",
    "    # Initialize CountVectorizer with specified parameters\n",
    "    cv = CountVectorizer(ngram_range=(1,2), lowercase=True, min_df=min_df, max_df=max_df, stop_words='english')\n",
    "    \n",
    "    # Fit CountVectorizer and transform text and title data\n",
    "    cv.fit(data.text_preproc)\n",
    "    vectorized_text = cv.transform(data.text_preproc).toarray()\n",
    "    vectorized_title = cv.transform(data.titles_preproc).toarray()\n",
    "    vectorized_text_title_weight = 2 * vectorized_title + vectorized_text\n",
    "    \n",
    "    # Apply SVD to CountVectorizer output\n",
    "    svd = TruncatedSVD(n_components=10)\n",
    "    U = svd.fit_transform(vectorized_text_title_weight)\n",
    "    VT = svd.components_\n",
    "    \n",
    "    # Initialize TfidfVectorizer with specified parameters\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=min_df, max_df=max_df)\n",
    "    \n",
    "    # Fit and transform text data using TfidfVectorizer\n",
    "    tfidf.fit(data.text_preproc)  # Adjusted to fit the correct dataset\n",
    "    tfidf_matrix_text = tfidf.transform(data.text_preproc)\n",
    "    tfidf_matrix_title = tfidf.transform(data.titles_preproc)\n",
    "    \n",
    "    # Perform SVD on TF-IDF matrices\n",
    "    svd_tfidf = TruncatedSVD(n_components=10)\n",
    "    U_text = svd_tfidf.fit_transform(tfidf_matrix_text)\n",
    "    U_title = svd_tfidf.transform(tfidf_matrix_title)\n",
    "    \n",
    "    # LDA Analysis\n",
    "    dtm = 2 * vectorized_title + vectorized_text  # Reuse weighted document-term matrix\n",
    "    dtm_sparse = csr_matrix(dtm)  # Convert to sparse format for gensim\n",
    "    corpus = Sparse2Corpus(dtm_sparse, documents_columns=False)  # Convert to gensim corpus\n",
    "    dictionary = Dictionary.from_corpus(corpus, id2word=dict((id, word) for word, id in cv.vocabulary_.items()))\n",
    "    \n",
    "    # Build LDA model\n",
    "    lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10, random_state=10)\n",
    "    \n",
    "    # Collect top words for LDA\n",
    "    lda_output = \"Top words associated with each topic (LDA):\\n\"\n",
    "    topics = lda_model.show_topics(num_words=10, formatted=False)\n",
    "    for topic_num, topic_words in topics:\n",
    "        words = ', '.join([word for word, _ in topic_words])\n",
    "        lda_output += f\"Topic {topic_num + 1}: {words}\\n\"\n",
    "\n",
    "    # Collect outputs for Count Vectorization and TF-IDF Vectorization\n",
    "    count_vectorization_output = \"\"\n",
    "    tfidf_vectorization_output = \"\"\n",
    "    \n",
    "    # Collect top words for Count Vectorization\n",
    "    count_vectorization_output += \"Top words associated with each topic (Count Vectorization):\\n\"\n",
    "    feature_names_count = cv.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(VT):\n",
    "        top_words_idx = topic.argsort()[::-1][:10]  # Get indices of top 10 words\n",
    "        top_words = [feature_names_count[i] for i in top_words_idx]\n",
    "        count_vectorization_output += f\"Topic {topic_idx+1}: {', '.join(top_words)}\\n\"\n",
    "    \n",
    "    # Collect top words for TF-IDF Vectorization\n",
    "    tfidf_vectorization_output += \"\\nTop words associated with each topic (TF-IDF Vectorization):\\n\"\n",
    "    feature_names_tfidf = tfidf.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(svd_tfidf.components_):\n",
    "        top_words_idx = topic.argsort()[::-1][:10]  # Get indices of top 10 words\n",
    "        top_words = [feature_names_tfidf[i] for i in top_words_idx]\n",
    "        tfidf_vectorization_output += f\"Topic {topic_idx+1}: {', '.join(top_words)}\\n\"    \n",
    "    \n",
    "    # Return all the collected outputs\n",
    "    return count_vectorization_output, tfidf_vectorization_output, lda_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_log_analyses(min_df_values, max_df_values, datasets, output_file_name):\n",
    "    with open(output_file_name, \"w\") as file:\n",
    "        for dataset_name, data in datasets.items():\n",
    "            file.write(f\"Analyzing dataset: {dataset_name}\\n\\n\")\n",
    "            for min_df in min_df_values:\n",
    "                for max_df in max_df_values:\n",
    "                    # Call analyze_text_data and unpack the returned outputs\n",
    "                    count_vectorization_output, tfidf_vectorization_output, lda_output = analyze_text_data(data, min_df, max_df)\n",
    "                    \n",
    "                    file.write(f\"min_df: {min_df}, max_df: {max_df}\\n\")\n",
    "                    file.write(count_vectorization_output + \"\\n\")\n",
    "                    file.write(tfidf_vectorization_output + \"\\n\")\n",
    "                    file.write(lda_output + \"\\n\\n\")\n",
    "            file.write(\"\\n\")  # Separator between datasets\n",
    "    print(f\"Analysis completed. Output written to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df_values = [0.005, 0.01, 0.05, 0.1]\n",
    "max_df_values = [0.2, 0.3]\n",
    "\n",
    "min_df_values_2 = [0.005, 0.01, 0.05, 0.1]\n",
    "max_df_values_2 = [0.4, 0.5, 0.6]\n",
    "\n",
    "datasets = {\n",
    "    \"joint_data\": joint_data\n",
    "}\n",
    "\n",
    "run_and_log_analyses(min_df_values, max_df_values, datasets, \"topics_analysis_output/analysis_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LDA model with the parameters we decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_with_lda(data, min_df, max_df):\n",
    "    # Initialize CountVectorizer\n",
    "    cv = CountVectorizer(ngram_range=(1, 2), lowercase=False, min_df=min_df, max_df=max_df)\n",
    "    \n",
    "    # Fit CountVectorizer and transform text data\n",
    "    cv.fit(data.text_preproc)\n",
    "    vectorized_text = cv.transform(data.text_preproc).toarray()\n",
    "    vectorized_title = cv.transform(data.titles_preproc).toarray()\n",
    "\n",
    "    # Combine text and title vectorized data with specified weight\n",
    "    dtm = 2 * vectorized_title + vectorized_text\n",
    "\n",
    "    # Convert the combined DTM to a sparse format for gensim\n",
    "    dtm_sparse = csr_matrix(dtm)\n",
    "\n",
    "    # Convert sparse matrix to gensim corpus\n",
    "    corpus = Sparse2Corpus(dtm_sparse, documents_columns=False)\n",
    "\n",
    "    # Create a Gensim dictionary from the CountVectorizer vocabulary\n",
    "    dictionary = Dictionary()\n",
    "    cv_vocabulary = {id_: token for token, id_ in cv.vocabulary_.items()}\n",
    "    # Manually populate the Dictionary object\n",
    "    dictionary.id2token = cv_vocabulary\n",
    "    dictionary.token2id = {token: id_ for id_, token in cv_vocabulary.items()}\n",
    "\n",
    "    # Hardcoded parameters for LDA\n",
    "    num_topics = 10\n",
    "    passes = 10\n",
    "    random_state = 10\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes, random_state=random_state)\n",
    "\n",
    "    # Display the topics\n",
    "    print(\"Top words associated with each topic (LDA):\")\n",
    "    topics = lda_model.show_topics(num_words=10, formatted=False)\n",
    "    for topic_num, topic_words in topics:\n",
    "        words = ', '.join([word for word, _ in topic_words])\n",
    "        print(f\"Topic {topic_num + 1}: {words}\")\n",
    "\n",
    "    return lda_model, corpus, cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_joint, corpus_joint, cv = analyze_text_with_lda(joint_data, 0.05, 0.5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
