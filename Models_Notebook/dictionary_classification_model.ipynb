{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.metrics as skm\n",
    "import re\n",
    "import numpy as np\n",
    "from string import digits\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel panel_a1\n",
    "panel_a1 = [\n",
    "    \"inflation expectation\", \"interest rate\", \"bank rate\", \"fund rate\", \"price\",\n",
    "    \"economic activity\", \"inflation\", \"employment\"\n",
    "]\n",
    "\n",
    "# Panel panel_a2\n",
    "panel_a2 = [\n",
    "    \"anchor\", \"cut\", \"subdue\", \"decline\", \"decrease\", \"reduce\", \"low\",\n",
    "    \"drop\", \"fall\", \"fell\", \"decelerate\", \"slow\", \"pause\", \"pausing\",\n",
    "    \"stable\", \"non-accelerating\", \"downward\", \"tighten\"\n",
    "]\n",
    "# Panel panel_b1\n",
    "panel_b1 = [\n",
    "    \"unemployment\", \"growth\", \"exchange rate\", \"productivity\", \"deficit\",\n",
    "    \"demand\", \"job market\", \"monetary policy\"\n",
    "]\n",
    "\n",
    "# Panel panel_b2\n",
    "panel_b2 = [\n",
    "    \"ease\", \"easing\", \"rise\", \"rising\", \"increase\", \"expand\", \"improve\",\n",
    "    \"strong\", \"upward\", \"raise\", \"high\", \"rapid\"\n",
    "]\n",
    "\n",
    "panel_c = [\"weren't\", \"were not\", \"wasn't\", \"was not\", 'did not', \"didn't\", \"do not\", \"don't\", 'will not', \"won't\"]\n",
    "\n",
    "\n",
    "def rule_model(df):\n",
    "    articles = df.tolist()\n",
    "    pred = []\n",
    "    for s in articles:\n",
    "        label = 0\n",
    "        if (any(word in s.lower() for word in panel_a1) and any(word in s.lower() for word in panel_a2)) or \\\n",
    "                (any(word in s.lower() for word in panel_b1) and any(word in s.lower() for word in panel_b2)):\n",
    "            label = 0\n",
    "        elif (any(word in s.lower() for word in panel_a1) and any(word in s.lower() for word in panel_b2)) or \\\n",
    "                (any(word in s.lower() for word in panel_b1) and any(word in s.lower() for word in panel_a2)):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 2\n",
    "        if label != 2 and (any(word in s.lower() for word in panel_c)):\n",
    "            pred.append(1 - label)  # turn 0 to 1, and 1 to 0\n",
    "        else:\n",
    "            pred.append(label)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles = pd.read_excel('OpenAI_Scored_Articles_3000.xlsx')\n",
    "\n",
    "# Use random sample of 1000 articles\n",
    "test_articles = test_articles.sample(1000, random_state=13)\n",
    "test_articles.dropna(inplace=True)\n",
    "test_articles['OpenAI_Score'] = test_articles['OpenAI_Score'].map({0: 'Neutral', -1: 'Dovish', 1: 'Hawkish'})\n",
    "test_articles.rename(columns={'Manual.summary': 'article', 'OpenAI_Score':'label'}, inplace=True)\n",
    "print(test_articles.shape)\n",
    "test_articles.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to perform the classification and calculate F1 score\n",
    "def classify_and_evaluate(seed):\n",
    "    # Read the data\n",
    "    test_articles = pd.read_excel('OpenAI_Scored_Articles_3000.xlsx')\n",
    "\n",
    "    # Use random sample of 1000 articles with specified seed\n",
    "    test_articles = test_articles.sample(1000, random_state=seed)\n",
    "    test_articles.dropna(inplace=True)\n",
    "    test_articles['OpenAI_Score'] = test_articles['OpenAI_Score'].map({0: 'Neutral', -1: 'Dovish', 1: 'Hawkish'})\n",
    "    test_articles.rename(columns={'Manual.summary': 'article', 'OpenAI_Score':'label'}, inplace=True)\n",
    "\n",
    "    # Apply rule-based model to predict labels\n",
    "    test_articles['dict_label'] = rule_model(test_articles['article'])\n",
    "    test_articles['dict_label'] = test_articles['dict_label'].map({0: 'Dovish', 1: 'Hawkish', 2: 'Neutral'})\n",
    "\n",
    "    # Compute accuracy\n",
    "    test_articles['Corrects'] = np.where(test_articles['dict_label'] == test_articles['label'], 1, 0)\n",
    "    accuracy = test_articles['Corrects'].sum() / len(test_articles)\n",
    "\n",
    "    # Encode labels for F1 score calculation\n",
    "    def encode_labels(labels):\n",
    "        label_map = {\"Hawkish\": 0, \"Dovish\": 1, \"Neutral\": 2}\n",
    "        return [label_map[label] for label in labels]\n",
    "\n",
    "    test_articles['label_encoded'] = encode_labels(test_articles['label'])\n",
    "    test_articles['dict_label_encoded'] = encode_labels(test_articles['dict_label'])\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(test_articles['label_encoded'], test_articles['dict_label_encoded'], average='weighted')\n",
    "\n",
    "    return accuracy, f1\n",
    "\n",
    "# Initialize variables to store best seed and corresponding f1 score\n",
    "best_seed = None\n",
    "best_f1_score = -1\n",
    "\n",
    "# Loop over different random seeds and compute F1 scores\n",
    "for seed in tqdm(range(1000)):\n",
    "    accuracy, f1 = classify_and_evaluate(seed)\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_seed = seed\n",
    "\n",
    "print(f\"Best seed: {best_seed}, Best F1 score: {best_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles['dict_label'] = rule_model(test_articles['article'])\n",
    "# Apply dictionary 0 is dovish, 1 is hawkish and 2 is neutral\n",
    "test_articles['dict_label'] = test_articles['dict_label'].map({0: 'Dovish', 1: 'Hawkish', 2: 'Neutral'})\n",
    "print(test_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Compare dict_label with Classification Rui to see how many are the same\n",
    "test_articles['Corrects'] = np.where(test_articles['dict_label'] == test_articles['label'], 1, 0)\n",
    "print(test_articles['Corrects'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversion function\n",
    "def encode_labels(labels):\n",
    "    label_map = {\"Hawkish\": 1, \"Dovish\": 0, \"Neutral\": 2}\n",
    "    return [label_map[label] for label in labels]\n",
    "\n",
    "# Example usage\n",
    "test_articles['label_encoded'] = encode_labels(test_articles['label'])\n",
    "test_articles['dict_label_encoded'] = encode_labels(test_articles['dict_label'])\n",
    "\n",
    "f1 = f1_score(test_articles['label_encoded'], test_articles['dict_label_encoded'], average='weighted')\n",
    "print(f\"F1 score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "actual_labels = test_articles['label_encoded']\n",
    "predicted_labels = test_articles['dict_label_encoded']\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=[0, 1, 2])\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(actual_labels, predicted_labels))\n",
    "\n",
    "# Create a test_dfframe for better visualization of the confusion matrix\n",
    "cm_df = pd.DataFrame(cm, index=['Actual_Dovish', 'Actual_Hawkish', 'Actual_Neutral'],\n",
    "                     columns=['Predicted_Dovish', 'Predicted_Hawkish', 'Predicted_Neutral'])\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Extracting TP, FP, TN, FN for each class\n",
    "# For 'Dovish'\n",
    "tp_dovish = cm[0, 0]\n",
    "fp_dovish = cm[1:, 0].sum()\n",
    "fn_dovish = cm[0, 1:].sum()\n",
    "tn_dovish = cm[1:, 1:].sum()\n",
    "\n",
    "# For 'Hawkish'\n",
    "tp_hawkish = cm[1, 1]\n",
    "fp_hawkish = cm[[0, 2], 1].sum()\n",
    "fn_hawkish = cm[1, [0, 2]].sum()\n",
    "tn_hawkish = cm[[0, 2], :][:, [0, 2]].sum()\n",
    "\n",
    "# For 'Neutral'\n",
    "tp_neutral = cm[2, 2]\n",
    "fp_neutral = cm[:2, 2].sum()\n",
    "fn_neutral = cm[2, :2].sum()\n",
    "tn_neutral = cm[:2, :2].sum()\n",
    "\n",
    "print(f\"Dovish - TP: {tp_dovish}, FP: {fp_dovish}, FN: {fn_dovish}, TN: {tn_dovish}\")\n",
    "print(f\"Hawkish - TP: {tp_hawkish}, FP: {fp_hawkish}, FN: {fn_hawkish}, TN: {tn_hawkish}\")\n",
    "print(f\"Neutral - TP: {tp_neutral}, FP: {fp_neutral}, FN: {fn_neutral}, TN: {tn_neutral}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
