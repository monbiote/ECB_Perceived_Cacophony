{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import newspaper\n",
    "import math\n",
    "import PyPDF2\n",
    "from pdfminer.high_level import extract_text\n",
    "import json\n",
    "\n",
    "data = pd.read_excel('combined.xlsx')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the base website for URL\n",
    "def extract_base_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "\n",
    "# Function to translate text to English\n",
    "def translate_to_english(text):\n",
    "    translator = Translator()\n",
    "    \n",
    "    # Check if text length is less than 5000 characters\n",
    "    if len(text) <= 5000:\n",
    "        try:\n",
    "            translated = translator.translate(text, dest='en')\n",
    "            return translated.text\n",
    "        except Exception as e:\n",
    "            print(\"Translation error:\", e)\n",
    "            return \"Translation error\"\n",
    "    else:\n",
    "        # Split text into smaller chunks\n",
    "        num_chunks = len(text) // 5000 + 1\n",
    "        chunks = [text[i*5000:(i+1)*5000] for i in range(num_chunks)]\n",
    "        \n",
    "        # Translate each chunk and concatenate the translations\n",
    "        translated_text = \"\"\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                translated = translator.translate(chunk, dest='en')\n",
    "                translated_text += translated.text + \" \"\n",
    "            except Exception as e:\n",
    "                print(\"Translation error:\", e)\n",
    "                return \"Translation error\"\n",
    "        \n",
    "        return translated_text.strip()\n",
    "\n",
    "def translate_greek_to_english(text):\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(text, src='el', dest='en')\n",
    "    return translated_text.text\n",
    "\n",
    "def split_text(text, num_segments):\n",
    "    # Calculate the length of each segment\n",
    "    segment_length = math.ceil(len(text) / num_segments)\n",
    "    # Split the text into segments\n",
    "    segments = [text[i:i+segment_length] for i in range(0, len(text), segment_length)]\n",
    "    return segments\n",
    "\n",
    "def translate_segments(segments):\n",
    "    translated_segments = []\n",
    "    for segment in segments:\n",
    "        translated_segment = translate_greek_to_english(segment)\n",
    "        translated_segments.append(translated_segment)\n",
    "    return translated_segments\n",
    "\n",
    "def combine_segments(segments):\n",
    "    return ''.join(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 45 most common website that we will use to scrape\n",
    "data['Website'] = data['Original.article.url'].apply(extract_base_url)\n",
    "data['Website'].value_counts().head(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "We decided to scrape the most common websites to get the most amount of full articles to work with\n",
    "## Problems\n",
    "#### bloomberg.com is payed\n",
    "#### handlesblatt.de doesn't work\n",
    "#### ilsusidiario.net doesnt't work\n",
    "#### di.se is payed or don't show anything\n",
    "#### boersen-zeitung.de is payed\n",
    "#### expansion.com is payed\n",
    "#### borsen.dk is payed\n",
    "#### jornaldenegocios.pt is payed\n",
    "#### borsa.corriere.it links don't work\n",
    "#### repubblica.it is payed\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reuters.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('reuters.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            # text = soup.find('div', class_='article-body')\n",
    "            # print(text)\n",
    "\n",
    "            # Find all paragraph elements with data-testid starting with 'paragraph-'\n",
    "            paragraphs = soup.find_all(lambda tag: tag.name == 'div' and tag.get('data-testid', '').startswith('paragraph-'))\n",
    "\n",
    "            # Extract text from each paragraph and print\n",
    "            text = \"\"\n",
    "            for paragraph in paragraphs:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except: \n",
    "            text = np.nan\n",
    "        print(text)\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.DataFrame({'url': urls, 'text': texts})\n",
    "    df.to_csv(\"Scraped/reuters.com.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For finanzen.ch links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('finanzen.ch')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            text = soup.find('div', class_='grid__col--12 instrument-description')\n",
    "\n",
    "            # Extract text and split paragraphs\n",
    "            paragraphs = [p.get_text(strip=True) for p in text.find_all('p')]\n",
    "\n",
    "            # Combine paragraphs into a single text with breaks\n",
    "            text = '\\n'.join(paragraphs)\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except: \n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Concatenate df with the new scraped data\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/finanzen.ch.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For milanofinanza.it links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('milanofinanza.it')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            text = soup.find('div', class_='newsContent corpo-articolo clearfix').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/milanofinanza.it.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For lavanguardia.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('lavanguardia.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='article-modules').find_all('p')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/lavanguardia.com.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For lainformacion.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('lainformacion.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='article-text').find_all(class_='paragraph')\n",
    "    \n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/lainformacion.com.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For finanza.lastampa.it links (No Paragraphs!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('finanza.lastampa.it')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='testoNews').find('div', class_=\"text-container\")\n",
    "\n",
    "            text = ''\n",
    "\n",
    "            # Iterate through the contents of the <p> tag\n",
    "            for content in a_tag.contents:\n",
    "                # Check if the content is a <strong> tag\n",
    "                if content.name == \"strong\":\n",
    "                    # If it's a <strong> tag, iterate through its contents\n",
    "                    for inner_content in content.contents:\n",
    "                        # Check if the inner content is a <br/> tag\n",
    "                        if inner_content.name == \"br\":\n",
    "                            # If it's a <br/> tag, add a newline character\n",
    "                            text += \"\\n\"\n",
    "                        else:\n",
    "                            # If it's text or another tag, add its text content\n",
    "                            text += str(inner_content)\n",
    "                else:\n",
    "                    # If it's not a <strong> tag, handle it as before\n",
    "                    if content.name == \"br\":\n",
    "                        # If it's a <br/> tag, add a newline character\n",
    "                        text += \"\\n\"\n",
    "                    else:\n",
    "                        # If it's text or another tag, add its text content\n",
    "                        text += str(content.text)\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/finanza.lastampa.it.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For europapress.es links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('europapress.es')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            text = soup.find('div', class_='NormalTextoNoticia').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/europapress.es.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For ilsole24ore.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('ilsole24ore.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='col-lg-10').find_all(class_='atext')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "                \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/ilsole24ore.com.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For ansa.it links (No Paragraphs!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('ansa.it')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            try: text = soup.find('div', class_='post-single-text rich-text').text.strip()\n",
    "            except: \n",
    "                try: text = soup.find('div', class_='news-txt').text.strip()\n",
    "                except: text = soup.find('div', class_='corpo').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/ansa.it.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For eleconomista.es links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('eleconomista.es')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='Article__paragraphGroup col-lg-10 col-md-12 col-12').find_all('p')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/eleconomista.es.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For expresso.pt links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('expresso.pt')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='full-article-fragment full-article-body article-content first').find_all('p')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/expresso.pt.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For bankingnews.gr links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('bankingnews.gr')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            try: a_tag = soup.find('div', class_='itemFullText').find('p')\n",
    "            except: a_tag = soup.find('div', class_='itemFullText')\n",
    "\n",
    "            text = \"\"\n",
    "\n",
    "            # Iterate through the contents of the <p> tag\n",
    "            for content in a_tag.contents:\n",
    "                # Check if the content is a <br/> tag\n",
    "                if content.name == \"br\":\n",
    "                    # If it's a <br/> tag, add a newline character\n",
    "                    text += \"\\n\"\n",
    "                else:\n",
    "                    # If it's text or another tag, add its text content\n",
    "                    text += str(content.text)\n",
    "\n",
    "            segments = split_text(text, 3)\n",
    "\n",
    "            # Translate each segment\n",
    "            translated_segments = translate_segments(segments)\n",
    "\n",
    "            # Combine the translated segments\n",
    "            text = combine_segments(translated_segments)\n",
    "            cleaned_text = re.sub(r'\\n(?!\\n)', ' ', text)\n",
    "            text = re.sub(r'\\n{2,}', '\\n', cleaned_text)\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/bankingnews.gr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For euro2day.gr links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('euro2day.gr')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='col-lg-10 col-md-11 article-sticky-container').find(class_='mt10').find_all('p')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "\n",
    "            segments = split_text(text, 3)\n",
    "\n",
    "            # Translate each segment\n",
    "            translated_segments = translate_segments(segments)\n",
    "\n",
    "            # Combine the translated segments\n",
    "            text = combine_segments(translated_segments)\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/euro2day.gr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For capital.gr links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('capital.gr')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', id='articleBody').find_all('p')\n",
    "            text = ''\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "            segments = split_text(text, 3)\n",
    "\n",
    "            # Translate each segment\n",
    "            translated_segments = translate_segments(segments)\n",
    "\n",
    "            # Combine the translated segments\n",
    "            text = combine_segments(translated_segments)\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/capital.gr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For cincodias.elpais.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('cincodias.elpais.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='a_c clearfix').find_all('p')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/cincodias.elpais.com.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For investor.bg links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('investor.bg')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='article-content')\n",
    "            b_tag = soup.find('div', class_='article-content').find(class_='related-articles')\n",
    "\n",
    "            # Find all <p> tags that are not within the related articles div\n",
    "            paragraphs_a = a_tag.find_all('p', recursive=False)\n",
    "\n",
    "            all_paragraphs_text = \"\"\n",
    "            for paragraph in paragraphs_a:\n",
    "                all_paragraphs_text += paragraph.text.strip() + \"\\n\"\n",
    "\n",
    "            cleaned_text = '\\n'.join(line for line in b_tag.text.strip().splitlines() if line.strip())\n",
    "\n",
    "            # Split both texts into lines\n",
    "            lines_a = all_paragraphs_text.splitlines()\n",
    "            lines_b = cleaned_text.splitlines()\n",
    "\n",
    "            # Find lines only in text_a while preserving order\n",
    "            remaining_lines = [line for line in lines_a if line not in lines_b]\n",
    "\n",
    "            # Join the remaining lines back into a string\n",
    "            text = '\\n'.join(remaining_lines)\n",
    "            text = translate_to_english(text)\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/investor.bg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For finanza.tgcom24.mediaset.it links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('finanza.tgcom24.mediaset.it')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            text = soup.find('div', class_='floatL w515 pleft5 t2 pT20').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "        break\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/finanza.tgcom24.mediaset.it.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For bfmtv.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('bfmtv.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='content_description')\n",
    "\n",
    "            if a_tag is None:\n",
    "                a_tag = soup.find('div', class_='content_body').find_all('p')\n",
    "\n",
    "                text = ''\n",
    "                for paragraph in a_tag:\n",
    "                    paragraph_text = paragraph.text.strip()\n",
    "                    text += paragraph_text + \"\\n\"\n",
    "\n",
    "            else:\n",
    "                text = a_tag.text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/bfmtv.com.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For faz.net links (Payed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('faz.net')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('article', class_='article')\n",
    "\n",
    "            script_tag = a_tag.find('script', type='application/ld+json')\n",
    "\n",
    "            if script_tag:\n",
    "                # Extract the JSON data from the <script> tag\n",
    "                json_data = script_tag.string\n",
    "                \n",
    "                # Parse the JSON data\n",
    "                parsed_data = json.loads(json_data)\n",
    "                \n",
    "                # Extract the value corresponding to the \"articleBody\" key\n",
    "                text = parsed_data.get(\"articleBody\", \"\")\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/faz.net.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For elconfidencial.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('elconfidencial.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            text = soup.find('div', class_='newsType__content').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/elconfidencial.com.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For challenges.fr links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('challenges.fr')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            text = soup.find('div', class_='corps').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/challenges.fr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For affaritaliani.it links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('affaritaliani.it')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            try: text = soup.find('div', class_='cnt-body').text.strip()\n",
    "            except: text = soup.find('div', itemprop='articleBody').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/affaritaliani.it.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For ilgiornale.it links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('ilgiornale.it')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            text = soup.find('div', class_='content__body').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/ilgiornale.it.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For abc.es links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('abc.es')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='voc-d').find_all(class_='voc-p')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/abc.es.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For wiwo.de links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('wiwo.de')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='o-article__content').find_all(class_='o-article__content-element')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/wiwo.de.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For jornaleconomico.pt links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('jornaleconomico.pt')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='uk-panel uk-margin').find_all('p')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/jornaleconomico.pt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For lesechos.fr links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('lesechos.fr')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='sc-1s859o0-0').find_all(class_='sc-nmm9yg-6 dTerhx')\n",
    "            b_tag = soup.find('div', class_='sc-1s859o0-0').find_all(class_='sc-nlqesd-4 cqTuwL')\n",
    "\n",
    "            paragraphs_a = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraphs_a = paragraphs_a + paragraph.text.strip() + '\\n'\n",
    "\n",
    "            paragraphs_b = \"\"\n",
    "            for paragraph in b_tag:\n",
    "                paragraphs_b = paragraphs_b + paragraph.text.strip() + '\\n'\n",
    "            paragraphs_b = paragraphs_b.replace(\"Lire aussi\\xa0:\", \"\")\n",
    "\n",
    "            # Split both texts into lines\n",
    "            lines_a = paragraphs_a.splitlines()\n",
    "            lines_b = paragraphs_b.splitlines()\n",
    "\n",
    "            # Convert lists to sets for faster comparison\n",
    "            set_a = set(lines_a)\n",
    "            set_b = set(lines_b)\n",
    "\n",
    "            # Find the set difference to get lines only in text_a\n",
    "            remaining_lines = [line for line in lines_a if line not in set_b]\n",
    "\n",
    "            # Join the remaining lines back into a string\n",
    "            text = '\\n'.join(remaining_lines)\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/lesechos.fr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For press.ruepointmedia.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('press.ruepointmedia.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='container').find('p').text.strip()\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(a_tag)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/press.ruepointmedia.com.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For elpais.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('/elpais.com')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='a_c').find_all('p')\n",
    "            \n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                if paragraph == a_tag[-1]:\n",
    "                    continue\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/elpais.com.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For trend.sk links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = data[data['Website'].str.contains('trend.sk')]['Original.article.url'].to_list()\n",
    "    \n",
    "    urls = []\n",
    "    texts = []\n",
    "\n",
    "    for URL in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', class_='article-body').find_all('p')\n",
    "\n",
    "            text = \"\"\n",
    "            for paragraph in a_tag:\n",
    "                paragraph_text = paragraph.text.strip()\n",
    "                text += paragraph_text + \"\\n\"\n",
    "        \n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        urls.append(URL)\n",
    "        texts.append(text)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'url': urls, 'text': texts})])\n",
    "    pd.DataFrame({'url': urls, 'text': texts}).to_csv(\"Scraped/trend.sk.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all the scraping together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing CSV files\n",
    "folder_path = 'Scraped/'\n",
    "# List to store individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "total_scraped_manual = pd.concat(dfs, ignore_index=True)\n",
    "total_scraped_manual = df.copy()\n",
    "total_scraped_manual = total_scraped_manual[total_scraped_manual['text']!='']\n",
    "total_scraped_manual.dropna(inplace=True)\n",
    "\n",
    "# Treat illegal characters\n",
    "# total_scraped_manual = total_scraped_manual.map(lambda x: x.encode('unicode_escape').\n",
    "#                 decode('utf-8') if isinstance(x, str) else x)\n",
    "# Create a new column with the base website URL\n",
    "total_scraped_manual['Website'] = total_scraped_manual['url'].apply(extract_base_url)\n",
    "total_scraped_manual.reset_index(inplace=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(total_scraped_manual.shape)\n",
    "total_scraped_manual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating the scraped text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_drop = []  # List to store indices of rows to drop\n",
    "\n",
    "for index, row in total_scraped_manual.iterrows():\n",
    "    text_value = row['text']\n",
    "    url_value = row['url']\n",
    "    if text_value is not None and isinstance(text_value, str) and text_value.strip() != '':\n",
    "        try:\n",
    "            detect(text_value)\n",
    "        except Exception as e:\n",
    "            print(f\"Error detected at index {index}: '{url_value} and {text_value}', Error: {e}\")\n",
    "            indices_to_drop.append(index)\n",
    "\n",
    "# Drop rows with indices where exceptions occurred\n",
    "total_scraped_manual = total_scraped_manual.drop(indices_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the language of the scraped text\n",
    "total_scraped_manual['Form'] = 'Manually'\n",
    "total_scraped_manual = total_scraped_manual.drop_duplicates(subset='url', keep='first')\n",
    "\n",
    "total_scraped_manual['language'] = total_scraped_manual['text'].apply(lambda x: detect(x) if x.strip() != '' else '')\n",
    "total_scraped_manual['length'] = total_scraped_manual['text'].apply(lambda x: len(x))\n",
    "\n",
    "print(total_scraped_manual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the length\n",
    "plt.hist(total_scraped_manual['length'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Length of Text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Text Lengths')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating all the text that's not in english to english\n",
    "# Got rid of texts that were too long because of capacity\n",
    "total_scraped_manual = total_scraped_manual[total_scraped_manual['length']<30000]\n",
    "\n",
    "# Assuming 'text_column' is the name of the column containing text in different languages\n",
    "total_scraped_manual['translated_text'] = np.where(total_scraped_manual['language']!='en', total_scraped_manual['text'].apply(translate_to_english), total_scraped_manual['text'])\n",
    "total_scraped_manual.to_csv('Translated/total_scraped_manual.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring a little of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_manual = pd.read_csv('Translated/total_scraped_manual.csv')\n",
    "total_manual = total_scraped_manual.copy()\n",
    "total_manual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the text in the first row of total_manual\n",
    "print(total_manual['translated_text'].iloc[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all text into a single string\n",
    "text = ' '.join(total_manual['translated_text'])\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stopwords corpus if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Combine all non-null text into a single string and replace \\n with spaces\n",
    "text = ' '.join(total_manual['translated_text'].dropna().astype(str)).replace('\\n', ' ')\n",
    "\n",
    "# Use regex to remove non-alphanumeric characters, numbers, and split into words\n",
    "words = re.findall(r'\\b[A-Za-z]{2,}\\b', text.lower())\n",
    "\n",
    "# Remove stopwords from the list of words\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# Get the top 20 most common words\n",
    "top_30_words = word_counts.most_common(30)\n",
    "\n",
    "# Print the top 20 most common words\n",
    "print(top_30_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 most common words that seem relevant\n",
    "top_words = [word[0] for word in top_30_words if word[0] not in ['also', 'would', 'de', 'last', 'one', 'new', 'types']]\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count occurrences of top words in text\n",
    "def count_top_words(text):\n",
    "    if pd.isnull(text):\n",
    "        return 0\n",
    "    # Tokenize the text into words\n",
    "    words = re.findall(r'\\b[A-Za-z]+\\b', text.lower())\n",
    "    # Count occurrences of top words in the text\n",
    "    count = sum(1 for word in words if word in top_words)\n",
    "    return count\n",
    "\n",
    "# Create a new column in the DataFrame to store the count of top words\n",
    "total_manual['top_words_count'] = total_manual['translated_text'].apply(count_top_words)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "total_manual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the top_words_count\n",
    "plt.hist(total_manual['top_words_count'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Top words count in text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Top words count in text')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_manual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the articles scrapped are good for our analysis, we will only use those that have at least 4 words of the most common list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_manual[total_manual['top_words_count'] < 4]['Website'].value_counts())\n",
    "# Remove rows with top_words_count less than 4\n",
    "total_manual = total_manual[total_manual['top_words_count'] >= 4]\n",
    "total_manual.drop(columns=['text', 'language', 'length'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For pdf links (paper.opoint.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download all pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link_type(url):\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(url)\n",
    "    \n",
    "    # Check if the URL path contains '/pdf/' or '.pdf'\n",
    "    if '/pdf/' in parsed_url.path or parsed_url.path.endswith('.pdf'):\n",
    "        return 'PDF'\n",
    "    \n",
    "    # Send a HEAD request to check Content-Type header\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        if 'application/pdf' in content_type:\n",
    "            return 'PDF'\n",
    "        elif 'text/html' in content_type:\n",
    "            return 'Website'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)\n",
    "        return 'Unknown'\n",
    "\n",
    "def download_pdf(url, folder, filename=None):\n",
    "    # Send a GET request to download the PDF\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # If filename is not provided, extract it from the URL\n",
    "            if filename is None:\n",
    "                filename = os.path.basename(urlparse(url).path)\n",
    "                # Remove invalid characters from the filename\n",
    "                filename = \"\".join(x for x in filename if x.isalnum() or x in ['.', '_', '-'])\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            # Save the PDF file\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "# List of URLs to check\n",
    "url_list = data['Original.article.url'].to_list()\n",
    "\n",
    "# Folder to save downloaded PDFs\n",
    "download_folder = 'pdfs'\n",
    "\n",
    "# Create the download folder if it doesn't exist\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)\n",
    "\n",
    "downloaded_pdfs = []\n",
    "i = 1\n",
    "# Iterate over each URL\n",
    "for url in url_list:\n",
    "    # Check the link type\n",
    "    link_type = check_link_type(url)\n",
    "    if link_type == 'PDF':\n",
    "        # Download the PDF into the folder\n",
    "        try: \n",
    "            download_pdf(url, download_folder, filename=f'{i}.pdf')\n",
    "            # Store the URL and PDF number tuple\n",
    "            downloaded_pdfs.append((url, i))\n",
    "            i += 1\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "# Convert the list of tuples into a DataFrame\n",
    "df_downloaded_pdfs = pd.DataFrame(downloaded_pdfs, columns=['URL', 'PDF_Number'])\n",
    "df_downloaded_pdfs.to_csv('pdfs/urls_pdfs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract text from pdf using PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text().strip()\n",
    "    return text\n",
    "\n",
    "# Directory containing PDF files\n",
    "pdfs_directory = 'pdfs'\n",
    "\n",
    "# Create an empty dictionary to store the text from the PDFs\n",
    "urls = []\n",
    "texts = []\n",
    "\n",
    "# Iterate over each row in the DataFrame to scrape the pdfs\n",
    "for index, row in df_downloaded_pdfs.iterrows():\n",
    "    # Get the URL and PDF number\n",
    "    url = row['URL']\n",
    "    pdf_number = row['PDF_Number']\n",
    "    # Get the full path of the PDF file\n",
    "    pdf_path = os.path.join(pdfs_directory, f'{pdf_number}.pdf')\n",
    "    # Extract text from the PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    # Translate the extracted text to English in case it's not in English\n",
    "    try:\n",
    "        if detect(text) != 'en':\n",
    "            text = translate_to_english(text)\n",
    "            text = unidecode(text)\n",
    "    except: text = np.nan\n",
    "    # Store the text in the dictionary using the url as the key\n",
    "    urls.append(url)\n",
    "    texts.append(text)\n",
    "    \n",
    "# Save the URLs and texts to a dataframe\n",
    "scraped_pdfs_df = pd.DataFrame({'url': urls, 'text': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the DataFrame to store the count of top words\n",
    "scraped_pdfs_df['top_words_count'] = scraped_pdfs_df['text'].apply(count_top_words)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "scraped_pdfs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the top_words_count\n",
    "plt.hist(scraped_pdfs_df['top_words_count'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Top words count in text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Top words count in text')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the urls that have less than 4 of the top words count\n",
    "failed_urls = scraped_pdfs_df[scraped_pdfs_df['top_words_count'] < 4]['url'].tolist()\n",
    "\n",
    "# Get the data frame from df_downloaded_pdfs that are in the pdfs_to_scrape list\n",
    "pdfs_to_scrape_df2 = df_downloaded_pdfs[df_downloaded_pdfs['URL'].isin(failed_urls)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract text from pdf using pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = extract_text(pdf_path)\n",
    "    # Replace line breaks with space\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "# Directory containing PDF files\n",
    "pdfs_directory = 'pdfs2'\n",
    "\n",
    "# Create an empty list to store the text from the PDFs\n",
    "urls = []\n",
    "texts = []\n",
    "\n",
    "# Iterate over each row in the DataFrame to scrape the PDFs\n",
    "for index, row in pdfs_to_scrape_df2.iterrows():\n",
    "    # Get the PDF number\n",
    "    url = row['URL']\n",
    "    pdf_number = row['PDF_Number']\n",
    "    # Get the full path of the PDF file\n",
    "    pdf_path = os.path.join(pdfs_directory, f'{pdf_number}.pdf')\n",
    "    # Extract text from the PDF\n",
    "    try: text = extract_text_from_pdf(pdf_path)\n",
    "    except: \n",
    "        text = np.nan\n",
    "    # Translate the extracted text to English in case it's not in English\n",
    "    try:\n",
    "        if detect(text) != 'en':\n",
    "            text = translate_to_english(text)\n",
    "            text = unidecode(text)\n",
    "    except: text = np.nan\n",
    "    # Append the text to the list\n",
    "    urls.append(url)\n",
    "    texts.append(text)\n",
    "    \n",
    "# Save the texts to a DataFrame along with the URLs\n",
    "scraped_pdfs_df2 = pd.DataFrame({'url': urls, 'text': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the DataFrame to store the count of top words\n",
    "scraped_pdfs_df2['top_words_count'] = scraped_pdfs_df2['text'].apply(count_top_words)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "scraped_pdfs_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the top_words_count\n",
    "plt.hist(scraped_pdfs_df2['top_words_count'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Top words count in text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Top words count in text')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the scraped pdfs with a top words count greater than 3\n",
    "final_pdfs_scraped = pd.concat([scraped_pdfs_df[scraped_pdfs_df['top_words_count'] > 3], scraped_pdfs_df2[scraped_pdfs_df2['top_words_count'] > 3]])\n",
    "final_pdfs_scraped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pdfs_scraped['translated_text'] = final_pdfs_scraped['text']\n",
    "final_pdfs_scraped.drop(columns=['text'], inplace=True)\n",
    "final_pdfs_scraped['Website'] = final_pdfs_scraped['url'].apply(extract_base_url)\n",
    "final_pdfs_scraped['Form'] = 'Manually'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate total_manual and final_pdfs_scraped\n",
    "total_manual = pd.concat([total_manual, final_pdfs_scraped], ignore_index=True)\n",
    "print(total_manual.shape)\n",
    "print(total_manual.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Newspaper3k to extract text from the remaining urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list_ready = total_manual['url'].tolist()\n",
    "\n",
    "# Remove all rows from data that have already been scraped\n",
    "articles_df = data[~data['Original.article.url'].isin(url_list_ready)]\n",
    "\n",
    "articles_links = articles_df['Original.article.url'].to_list()\n",
    "languages = articles_df['Language']\n",
    "\n",
    "articles_data = []\n",
    "valid_language_codes = ['ar', 'ru', 'nl', 'de', 'en', 'es', 'fr', 'he', 'it', 'ko', 'no', 'fa', 'pl', 'pt', 'sv', 'hu', 'fi', 'da', 'zh', 'id', 'vi', 'sw', 'tr', 'el', 'uk']\n",
    "\n",
    "for url, lang in zip(articles_links, languages):\n",
    "    if lang in valid_language_codes:\n",
    "        article = newspaper.Article(url=url, language = lang)\n",
    "    else:\n",
    "        article = newspaper.Article(url=url)\n",
    "    try:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = str(article.text)\n",
    "        try:\n",
    "            if detect(text) != 'en':\n",
    "                text = translate_to_english(text)\n",
    "                text = unidecode(text)\n",
    "        except: text = np.nan\n",
    "\n",
    "        article_data = {\n",
    "            \"url\": url,\n",
    "            \"text\": text\n",
    "        }\n",
    "        articles_data.append(article_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        pass\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "df.to_csv('Newspaper/data_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Newspaper/data_articles.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \\n for spaces in text\n",
    "df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "\n",
    "# Get the language of the scraped text\n",
    "df['Form'] = 'Newspaper3k'\n",
    "df = df.drop_duplicates(subset='url', keep='first')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['language'] = df['text'].apply(lambda x: detect(x) if x.strip() != '' else '')\n",
    "df['length'] = df['text'].apply(lambda x: len(x))\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the length\n",
    "plt.hist(df['length'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Length of Text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Text Lengths')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating all the text that's not in english to english\n",
    "# Got rid of texts that were too long because of capacity\n",
    "df = df[df['length']<30000]\n",
    "\n",
    "# Treat illegal characters\n",
    "df = df.map(lambda x: x.encode('unicode_escape').\n",
    "                decode('utf-8') if isinstance(x, str) else x)\n",
    "\n",
    "# Function to translate text to English\n",
    "def translate_to_english(text):\n",
    "    translator = Translator()\n",
    "    \n",
    "    # Check if text length is less than 5000 characters\n",
    "    if len(text) <= 5000:\n",
    "        try:\n",
    "            translated = translator.translate(text, dest='en')\n",
    "            return translated.text\n",
    "        except Exception as e:\n",
    "            print(\"Translation error:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        # Split text into smaller chunks\n",
    "        num_chunks = len(text) // 5000 + 1\n",
    "        chunks = [text[i*5000:(i+1)*5000] for i in range(num_chunks)]\n",
    "        \n",
    "        # Translate each chunk and concatenate the translations\n",
    "        translated_text = \"\"\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                translated = translator.translate(chunk, dest='en')\n",
    "                translated_text += translated.text + \" \"\n",
    "            except Exception as e:\n",
    "                print(\"Translation error:\", e)\n",
    "        \n",
    "        return translated_text.strip()\n",
    "\n",
    "# Assuming 'text_column' is the name of the column containing text in different languages\n",
    "df['translated_text'] = np.where(df['language']!='en', df['text'].apply(translate_to_english), df['text'])\n",
    "\n",
    "df.to_csv('Translated/translated_newspaper.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_newspaper = pd.read_csv('Translated/translated_newspaper.csv')\n",
    "translated_newspaper['Website'] = translated_newspaper['url'].apply(extract_base_url)\n",
    "translated_newspaper['top_words_count'] = translated_newspaper['translated_text'].dropna().apply(count_top_words)\n",
    "translated_newspaper.drop(columns=['text', 'language', 'length'], inplace=True)\n",
    "translated_newspaper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the final_df of the manual scraping and translated_newspaper\n",
    "final_df = pd.concat([total_manual, translated_newspaper], ignore_index=True)\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all the rows that have a top words count less than 4\n",
    "final_df = final_df[final_df['top_words_count'] >= 4]\n",
    "final_df.to_excel('final_total_df.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
