{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The webpage URL\n",
    "    URL = \"https://www.ecb.europa.eu/press/pr/activities/mopo/html/index.en.html\"\n",
    "    \n",
    "    # HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    date_list = []\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "    article_list = []\n",
    "\n",
    "    # Fetch dates, names and links as lists\n",
    "    dates = soup.find_all(\"div\", attrs={'class':'date'})\n",
    "    titles = soup.find_all(\"div\", attrs={'class':'category'})\n",
    "    links = soup.find_all('dd')\n",
    "\n",
    "    for date in dates:\n",
    "        date_list.append(date.text)\n",
    "    \n",
    "    for title in titles:\n",
    "        title_list.append(title.text)\n",
    "\n",
    "    for link in links:\n",
    "        link_list.append('https://www.ecb.europa.eu/' + link.find('div', class_='title').find('a').get('href'))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the lists\n",
    "df = pd.DataFrame(list(zip(date_list, title_list, link_list)), columns=['Date', 'Title', 'Link'])\n",
    "\n",
    "# Filter to keep only those with Title MONETARY POLICY DECISION and Date from 2021 onward\n",
    "df = df[df['Title'] == 'MONETARY POLICY DECISION']\n",
    "df = df[df['Date'].apply(lambda x: datetime.strptime(x, '%d %B %Y').year >= 2021)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; ru) Opera 8.0', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The base webpage URL\n",
    "    url_list = df['Link'].tolist()\n",
    "    article_list = []\n",
    "\n",
    "    for link in url_list:\n",
    "        try:\n",
    "            # HTTP Request\n",
    "            webpage = requests.get(link, headers=HEADERS)\n",
    "\n",
    "            # Soup Object containing all data\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            a_tag = soup.find('div', id='main-wrapper')\n",
    "            text = a_tag.find('div', class_='section').text.strip()\n",
    "\n",
    "        except: text = np.nan\n",
    "\n",
    "        try: text = unidecode(text)\n",
    "        except:\n",
    "            text = np.nan\n",
    "        article_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match the date at the beginning of each text\n",
    "date_pattern = r'^\\d+\\s+\\w+\\s+\\d+'\n",
    "\n",
    "# Remove the date from each text using re.sub()\n",
    "cleaned_articles = [re.sub(date_pattern, '', text).lstrip() for text in article_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cleaned articles to the DataFrame\n",
    "df['Article'] = cleaned_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change date format 11 April 2024 to 2024-04-11\n",
    "df['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, '%d %B %Y').strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to keep only those between July 2022 and March 2024\n",
    "df = df[df['Date'].apply(lambda x: '2022-07-01' <= x <= '2024-03-31')]\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
