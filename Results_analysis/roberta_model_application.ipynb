{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with OPEN AI SCORE and DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.read_csv('/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/df_cleaned_final_01.csv')\n",
    "df_scores_gpt4=pd.read_excel('/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/Scored_Articles_3000.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicates for manual.summary in df_final\n",
    "\n",
    "df_final['Manual.summary'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scores_gpt4.columns)\n",
    "print(df_scores_gpt4.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are matches between the two dataframes that exceed, it is because of the duplicates that we decide to kept if they were in different plataforms\n",
    "\n",
    "# Check the number of matches\n",
    "matches = df_final['Manual.summary'].isin(df_scores_gpt4['Manual.summary'])\n",
    "number_of_matches = matches.sum()\n",
    "\n",
    "print(f\"Number of matching entries: {number_of_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  From what I am understanding these are things that are non matched in our dataframe, so they were dropped before the final dataframe, but they were still in the random ones. \n",
    "#  Check the number of matches\n",
    "matches = df_scores_gpt4['Manual.summary'].isin(df_final['Manual.summary'])\n",
    "number_of_matches = matches.sum()\n",
    "\n",
    "print(f\"Number of matching entries: {number_of_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)  # Set the display width to 1000 characters wide\n",
    "pd.set_option('display.max_colwidth', None)  # Remove truncation of column content\n",
    "\n",
    "\n",
    "# Check for non-matching entries\n",
    "non_matches = ~df_scores_gpt4['Manual.summary'].isin(df_final['Manual.summary'])\n",
    "non_matching_entries = df_scores_gpt4[non_matches]\n",
    "\n",
    "# Print the number of non-matching entries\n",
    "print(f\"Number of non-matching entries: {non_matching_entries.shape[0]}\")\n",
    "\n",
    "# Display the non-matching entries\n",
    "if non_matching_entries.shape[0] > 0:\n",
    "    print(\"Non-matching entries in 'Manual.summary' from df_scores_gpt4:\")\n",
    "    print(non_matching_entries['Manual.summary'])\n",
    "else:\n",
    "    print(\"No non-matching entries found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset options every option\n",
    "pd.reset_option('all')\n",
    "\n",
    "# First, check the number of matches before merging\n",
    "matches = df_final['Manual.summary'].isin(df_scores_gpt4['Manual.summary'])\n",
    "number_of_matches_before_merge = matches.sum()\n",
    "print(f\"Number of matches before merge: {number_of_matches_before_merge}\")\n",
    "\n",
    "# Perform the merge to add the 'OpenAI_Score' from df_scores_gpt4 to df_final\n",
    "# Make sure there's no column name conflict by specifying suffixes if needed\n",
    "df_final = df_final.merge(df_scores_gpt4[['Manual.summary', 'OpenAI_Score']],\n",
    "                          on='Manual.summary', how='left', suffixes=('', '_drop'))\n",
    "\n",
    "# After merge, drop any duplicate or unnecessary columns that may have resulted from the merge\n",
    "df_final.drop([col for col in df_final.columns if 'drop' in col], axis=1, inplace=True)\n",
    "\n",
    "# Print the result of the merge to verify\n",
    "df_final.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of values in the 'OpenAI_Score' column\n",
    "total_values = len(df_final['OpenAI_Score'])\n",
    "print(f\"Total number of values in 'OpenAI_Score': {total_values}\")\n",
    "\n",
    "# Print the number of non-null values in the 'OpenAI_Score' column\n",
    "non_null_values = df_final['OpenAI_Score'].count()\n",
    "print(f\"Number of non-null values in 'OpenAI_Score': {non_null_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing the score based on the Open_AI Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns only that are non-null in OPENAI_SCORE\n",
    "df_open_ai_scores= df_final[df_final['OpenAI_Score'].notnull()]\n",
    "df_open_ai_scores.to_csv('/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/df_open_ai_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train of the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the fine-tuned model and tokenizer\n",
    "model_path = \"/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/finetuned-FOMC-RoBERTa\"\n",
    "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Create a pipeline for sentiment analysis\n",
    "classifier = pipeline(\"sentiment-analysis\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\n",
    "\n",
    "# Select only the first 100 rows for processing\n",
    "unlabeled_texts = df_final['Manual.summary'].head(1000).tolist()\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier(unlabeled_texts)\n",
    "\n",
    "# Add predictions to the dataframe for the first 100 rows\n",
    "df_final.loc[:999, 'bert_predictions_test'] = [pred['label'] for pred in predictions]  # using loc to ensure we only modify the first 100 rows\n",
    "\n",
    "# Display the updated part of the dataframe\n",
    "print(df_final.head(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df_final['bert_predictions_test'].value_counts(dropna=True)\n",
    "print(unique_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df_final['bert_predictions_test'].value_counts(dropna=True)\n",
    "print(unique_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)  # Set the display width to 1000 characters wide\n",
    "pd.set_option('display.max_colwidth', None)  # Remove truncation of column content\n",
    "\n",
    "#Print examples of Manual.Summary Label_1 predictions\n",
    "print(df_final['Manual.summary'][df_final['bert_predictions_test'] == 'LABEL_2'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final['Manual.summary'][df_final['bert_predictions_test'] == 'LABEL_1'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final['Manual.summary'][df_final['bert_predictions_test'] == 'LABEL_0'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test to solve the issue from the previous run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/finetuned-FOMC-RoBERTa\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Setup the pipeline for sentiment analysis using the loaded model and tokenizer\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# DataFrame column to list\n",
    "summaries = df_final['Manual.summary'].tolist()\n",
    "\n",
    "# Error indices identified from previous runs\n",
    "error_indices = [1785, 7148, 7780]\n",
    "ranges_to_process = []\n",
    "\n",
    "# Calculate ranges: 10 before, the index itself, and 10 after\n",
    "for index in error_indices:\n",
    "    start = max(0, index - 10)\n",
    "    end = min(len(summaries), index + 11)\n",
    "    ranges_to_process.append(range(start, end))\n",
    "\n",
    "# Process the data in these specific ranges\n",
    "predictions = [None] * len(summaries)  # Initialize a list of None to store predictions\n",
    "for range_to_process in ranges_to_process:\n",
    "    for i in range_to_process:\n",
    "        try:\n",
    "            # Make predictions for the current summary\n",
    "            batch_predictions = classifier([summaries[i]])\n",
    "            predictions[i] = batch_predictions[0]['label']\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing summary at index {i}: {e}\")\n",
    "            predictions[i] = 'Error'  # Mark this as an error\n",
    "\n",
    "# Safely add predictions to the dataframe ensuring all are accounted for\n",
    "df_final['bert_predictions_everything'] = predictions\n",
    "\n",
    "# Display part of the updated dataframe\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df_final['bert_predictions_everything'].value_counts(dropna=True)\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying this for the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/finetuned-FOMC-RoBERTa\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Setup the pipeline for sentiment analysis using the loaded model and tokenizer\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# DataFrame column to list\n",
    "summaries = df_final['Manual.summary'].tolist()\n",
    "\n",
    "# Initialize a predictions list\n",
    "predictions = []\n",
    "\n",
    "# Process each summary individually\n",
    "for i, summary in enumerate(summaries):\n",
    "    try:\n",
    "        # Make predictions for the current summary\n",
    "        batch_predictions = classifier([summary])  # Processing one summary at a time\n",
    "        predictions.extend(batch_predictions)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing summary at index {i}: {e}\")\n",
    "        predictions.append({'label': 'Error'})  # Mark this as an error\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "df_final['bert_predictions_everything'] = [pred['label'] for pred in predictions]\n",
    "\n",
    "# Display part of the updated dataframe\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df_final['bert_predictions_everything'].value_counts(dropna=True)\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total counts for each label\n",
    "label_1_count = 23722\n",
    "label_0_count = 5224\n",
    "label_2_count = 1211\n",
    "\n",
    "# Total counts combined\n",
    "total_count = label_1_count + label_0_count + label_2_count\n",
    "\n",
    "# Calculating percentages for each label\n",
    "percentage_label_1 = (label_1_count / total_count) * 100\n",
    "percentage_label_0 = (label_0_count / total_count) * 100\n",
    "percentage_label_2 = (label_2_count / total_count) * 100\n",
    "\n",
    "percentage_label_1, percentage_label_0, percentage_label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('/Users/ruimaciel/Desktop/Local_ECB_Cacophony_Master_Thesis/df_final_with_bert_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
